{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T3: Neural IBM1 (with collocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first run a few imports:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from utils import smart_reader, bitext_reader\n",
    "from vocabulary import OrderedCounter, Vocabulary \n",
    "from utils import iterate_minibatches, prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the paths to our training and validation data, English side\n",
    "train_e_path = 'data/training/hansards.36.2.e.gz'\n",
    "train_f_path = 'data/training/hansards.36.2.f.gz'\n",
    "dev_e_path = 'data/validation/dev.e.gz'\n",
    "dev_f_path = 'data/validation/dev.f.gz'\n",
    "dev_wa = 'data/validation/dev.wa.nonullalign'\n",
    "test_e_path = 'data/test/test.e.gz'\n",
    "test_f_path = 'data/test/test.f.gz'\n",
    "test_wa = 'data/test/test.wa.nonullalign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using only 1000 words will result in many UNKs, but\n",
    "# it will make training a lot faster. \n",
    "# If you have a fast computer, a GPU, or a lot of time,\n",
    "# try with 10000 instead.\n",
    "max_tokens=1000\n",
    "\n",
    "corpus_e = smart_reader(train_e_path)    \n",
    "vocabulary_e = Vocabulary(corpus=corpus_e, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_e, open(\"vocabulary_e.pkl\", mode=\"wb\"))\n",
    "\n",
    "corpus_f = smart_reader(train_f_path)    \n",
    "vocabulary_f = Vocabulary(corpus=corpus_f, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_f, open(\"vocabulary_f.pkl\", mode=\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralibm1 import NeuralIBM1Model\n",
    "from neuralibm1trainer import NeuralIBM1Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with B=16 max_length=30 lr=0.001 lr_decay=0.0\n",
      "Initializing variables..\n",
      "Training started..\n",
      "Shuffling training data\n",
      "Iter   100 loss 68.553726 accuracy 0.16 lr 0.001000\n",
      "Iter   200 loss 58.562141 accuracy 0.20 lr 0.001000\n",
      "Iter   300 loss 51.571232 accuracy 0.18 lr 0.001000\n",
      "Iter   400 loss 56.012936 accuracy 0.18 lr 0.001000\n",
      "Iter   500 loss 64.196602 accuracy 0.17 lr 0.001000\n",
      "Iter   600 loss 50.099098 accuracy 0.20 lr 0.001000\n",
      "Iter   700 loss 45.803978 accuracy 0.20 lr 0.001000\n",
      "Iter   800 loss 53.881210 accuracy 0.22 lr 0.001000\n",
      "Iter   900 loss 64.321457 accuracy 0.17 lr 0.001000\n",
      "Iter  1000 loss 51.318100 accuracy 0.21 lr 0.001000\n",
      "Iter  1100 loss 58.482121 accuracy 0.19 lr 0.001000\n",
      "Iter  1200 loss 39.769440 accuracy 0.22 lr 0.001000\n",
      "Iter  1300 loss 51.398457 accuracy 0.13 lr 0.001000\n",
      "Iter  1400 loss 49.927032 accuracy 0.19 lr 0.001000\n",
      "Iter  1500 loss 48.800968 accuracy 0.21 lr 0.001000\n",
      "Iter  1600 loss 48.834038 accuracy 0.18 lr 0.001000\n",
      "Iter  1700 loss 64.203423 accuracy 0.16 lr 0.001000\n",
      "Iter  1800 loss 40.611019 accuracy 0.26 lr 0.001000\n",
      "Iter  1900 loss 45.720776 accuracy 0.18 lr 0.001000\n",
      "Iter  2000 loss 55.279968 accuracy 0.18 lr 0.001000\n",
      "Iter  2100 loss 53.307777 accuracy 0.20 lr 0.001000\n",
      "Iter  2200 loss 40.644875 accuracy 0.19 lr 0.001000\n",
      "Iter  2300 loss 50.050789 accuracy 0.15 lr 0.001000\n",
      "Iter  2400 loss 62.514709 accuracy 0.17 lr 0.001000\n",
      "Iter  2500 loss 45.390793 accuracy 0.18 lr 0.001000\n",
      "Iter  2600 loss 51.178833 accuracy 0.19 lr 0.001000\n",
      "Iter  2700 loss 51.922531 accuracy 0.18 lr 0.001000\n",
      "Iter  2800 loss 38.520580 accuracy 0.19 lr 0.001000\n",
      "Iter  2900 loss 44.907669 accuracy 0.17 lr 0.001000\n",
      "Iter  3000 loss 34.246502 accuracy 0.18 lr 0.001000\n",
      "Iter  3100 loss 37.379517 accuracy 0.18 lr 0.001000\n",
      "Iter  3200 loss 47.397705 accuracy 0.24 lr 0.001000\n",
      "Iter  3300 loss 48.723286 accuracy 0.19 lr 0.001000\n",
      "Iter  3400 loss 49.368370 accuracy 0.21 lr 0.001000\n",
      "Iter  3500 loss 48.293705 accuracy 0.21 lr 0.001000\n",
      "Iter  3600 loss 47.873940 accuracy 0.17 lr 0.001000\n",
      "Iter  3700 loss 53.657719 accuracy 0.16 lr 0.001000\n",
      "Iter  3800 loss 41.560791 accuracy 0.19 lr 0.001000\n",
      "Iter  3900 loss 43.426872 accuracy 0.21 lr 0.001000\n",
      "Iter  4000 loss 46.306320 accuracy 0.21 lr 0.001000\n",
      "Iter  4100 loss 32.321552 accuracy 0.15 lr 0.001000\n",
      "Iter  4200 loss 55.201805 accuracy 0.18 lr 0.001000\n",
      "Iter  4300 loss 48.402256 accuracy 0.19 lr 0.001000\n",
      "Iter  4400 loss 52.762245 accuracy 0.21 lr 0.001000\n",
      "Iter  4500 loss 48.545631 accuracy 0.22 lr 0.001000\n",
      "Iter  4600 loss 35.998566 accuracy 0.20 lr 0.001000\n",
      "Iter  4700 loss 32.827240 accuracy 0.28 lr 0.001000\n",
      "Iter  4800 loss 53.375282 accuracy 0.16 lr 0.001000\n",
      "Iter  4900 loss 44.834663 accuracy 0.20 lr 0.001000\n",
      "Iter  5000 loss 60.978901 accuracy 0.18 lr 0.001000\n",
      "Iter  5100 loss 50.278183 accuracy 0.22 lr 0.001000\n",
      "Iter  5200 loss 47.193867 accuracy 0.17 lr 0.001000\n",
      "Iter  5300 loss 47.662472 accuracy 0.17 lr 0.001000\n",
      "Iter  5400 loss 69.854378 accuracy 0.19 lr 0.001000\n",
      "Iter  5500 loss 38.502731 accuracy 0.23 lr 0.001000\n",
      "Iter  5600 loss 54.236130 accuracy 0.17 lr 0.001000\n",
      "Iter  5700 loss 47.066689 accuracy 0.18 lr 0.001000\n",
      "Iter  5800 loss 55.538628 accuracy 0.19 lr 0.001000\n",
      "Iter  5900 loss 53.026344 accuracy 0.19 lr 0.001000\n",
      "Iter  6000 loss 41.195488 accuracy 0.17 lr 0.001000\n",
      "Iter  6100 loss 39.226463 accuracy 0.17 lr 0.001000\n",
      "Iter  6200 loss 60.204330 accuracy 0.18 lr 0.001000\n",
      "Iter  6300 loss 45.515026 accuracy 0.16 lr 0.001000\n",
      "Iter  6400 loss 29.564693 accuracy 0.24 lr 0.001000\n",
      "Iter  6500 loss 68.102097 accuracy 0.14 lr 0.001000\n",
      "Iter  6600 loss 43.937466 accuracy 0.17 lr 0.001000\n",
      "Iter  6700 loss 42.277527 accuracy 0.18 lr 0.001000\n",
      "Iter  6800 loss 37.999783 accuracy 0.23 lr 0.001000\n",
      "Iter  6900 loss 35.758308 accuracy 0.17 lr 0.001000\n",
      "Iter  7000 loss 49.106083 accuracy 0.17 lr 0.001000\n",
      "Iter  7100 loss 39.382881 accuracy 0.21 lr 0.001000\n",
      "Iter  7200 loss 39.103065 accuracy 0.20 lr 0.001000\n",
      "Iter  7300 loss 36.183113 accuracy 0.20 lr 0.001000\n",
      "Iter  7400 loss 48.944237 accuracy 0.19 lr 0.001000\n",
      "Iter  7500 loss 61.973129 accuracy 0.22 lr 0.001000\n",
      "Iter  7600 loss 49.316437 accuracy 0.23 lr 0.001000\n",
      "Iter  7700 loss 46.261673 accuracy 0.24 lr 0.001000\n",
      "Iter  7800 loss 53.340714 accuracy 0.17 lr 0.001000\n",
      "Iter  7900 loss 45.826988 accuracy 0.17 lr 0.001000\n",
      "Iter  8000 loss 51.898952 accuracy 0.16 lr 0.001000\n",
      "Iter  8100 loss 39.178520 accuracy 0.23 lr 0.001000\n",
      "Iter  8200 loss 47.900597 accuracy 0.20 lr 0.001000\n",
      "Iter  8300 loss 38.950676 accuracy 0.18 lr 0.001000\n",
      "Iter  8400 loss 38.040718 accuracy 0.20 lr 0.001000\n",
      "Iter  8500 loss 53.131851 accuracy 0.17 lr 0.001000\n",
      "Iter  8600 loss 33.710434 accuracy 0.21 lr 0.001000\n",
      "Iter  8700 loss 41.332001 accuracy 0.18 lr 0.001000\n",
      "Iter  8800 loss 51.737152 accuracy 0.19 lr 0.001000\n",
      "Iter  8900 loss 26.869556 accuracy 0.22 lr 0.001000\n",
      "Iter  9000 loss 43.353622 accuracy 0.22 lr 0.001000\n",
      "Iter  9100 loss 48.850109 accuracy 0.20 lr 0.001000\n",
      "Iter  9200 loss 31.262455 accuracy 0.20 lr 0.001000\n",
      "Iter  9300 loss 42.841892 accuracy 0.23 lr 0.001000\n",
      "Iter  9400 loss 49.303509 accuracy 0.17 lr 0.001000\n",
      "Iter  9500 loss 38.313278 accuracy 0.25 lr 0.001000\n",
      "Iter  9600 loss 58.846222 accuracy 0.19 lr 0.001000\n",
      "Iter  9700 loss 34.418640 accuracy 0.23 lr 0.001000\n",
      "Iter  9800 loss 56.566967 accuracy 0.20 lr 0.001000\n",
      "Iter  9900 loss 37.117138 accuracy 0.17 lr 0.001000\n",
      "Iter 10000 loss 44.760735 accuracy 0.21 lr 0.001000\n",
      "Iter 10100 loss 29.013145 accuracy 0.22 lr 0.001000\n",
      "Iter 10200 loss 32.341209 accuracy 0.28 lr 0.001000\n",
      "Iter 10300 loss 44.645214 accuracy 0.22 lr 0.001000\n",
      "Iter 10400 loss 56.620892 accuracy 0.18 lr 0.001000\n",
      "Iter 10500 loss 53.868568 accuracy 0.19 lr 0.001000\n",
      "Iter 10600 loss 29.320412 accuracy 0.22 lr 0.001000\n",
      "Iter 10700 loss 42.453705 accuracy 0.19 lr 0.001000\n",
      "Iter 10800 loss 48.489014 accuracy 0.19 lr 0.001000\n",
      "Iter 10900 loss 54.479237 accuracy 0.17 lr 0.001000\n",
      "Iter 11000 loss 39.240097 accuracy 0.22 lr 0.001000\n",
      "Iter 11100 loss 68.223724 accuracy 0.18 lr 0.001000\n",
      "Epoch 1 loss 46.839978 accuracy 0.20 val_aer 0.91 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 50.022354 accuracy 0.19 lr 0.001000\n",
      "Iter   200 loss 31.997004 accuracy 0.27 lr 0.001000\n",
      "Iter   300 loss 40.475327 accuracy 0.19 lr 0.001000\n",
      "Iter   400 loss 39.662102 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss 42.560528 accuracy 0.24 lr 0.001000\n",
      "Iter   600 loss 26.878948 accuracy 0.20 lr 0.001000\n",
      "Iter   700 loss 57.471481 accuracy 0.17 lr 0.001000\n",
      "Iter   800 loss 53.206345 accuracy 0.17 lr 0.001000\n",
      "Iter   900 loss 45.829296 accuracy 0.20 lr 0.001000\n",
      "Iter  1000 loss 46.224220 accuracy 0.23 lr 0.001000\n",
      "Iter  1100 loss 55.353405 accuracy 0.17 lr 0.001000\n",
      "Iter  1200 loss 38.048077 accuracy 0.21 lr 0.001000\n",
      "Iter  1300 loss 51.777802 accuracy 0.20 lr 0.001000\n",
      "Iter  1400 loss 51.102314 accuracy 0.20 lr 0.001000\n",
      "Iter  1500 loss 53.205544 accuracy 0.20 lr 0.001000\n",
      "Iter  1600 loss 42.005119 accuracy 0.20 lr 0.001000\n",
      "Iter  1700 loss 40.259354 accuracy 0.24 lr 0.001000\n",
      "Iter  1800 loss 47.980789 accuracy 0.17 lr 0.001000\n",
      "Iter  1900 loss 27.901703 accuracy 0.20 lr 0.001000\n",
      "Iter  2000 loss 37.755497 accuracy 0.18 lr 0.001000\n",
      "Iter  2100 loss 50.676525 accuracy 0.19 lr 0.001000\n",
      "Iter  2200 loss 47.487885 accuracy 0.23 lr 0.001000\n",
      "Iter  2300 loss 52.806259 accuracy 0.18 lr 0.001000\n",
      "Iter  2400 loss 48.245174 accuracy 0.20 lr 0.001000\n",
      "Iter  2500 loss 43.939655 accuracy 0.19 lr 0.001000\n",
      "Iter  2600 loss 45.458076 accuracy 0.24 lr 0.001000\n",
      "Iter  2700 loss 46.883858 accuracy 0.18 lr 0.001000\n",
      "Iter  2800 loss 52.125648 accuracy 0.22 lr 0.001000\n",
      "Iter  2900 loss 50.073204 accuracy 0.18 lr 0.001000\n",
      "Iter  3000 loss 47.738777 accuracy 0.22 lr 0.001000\n",
      "Iter  3100 loss 39.589203 accuracy 0.23 lr 0.001000\n",
      "Iter  3200 loss 39.504192 accuracy 0.18 lr 0.001000\n",
      "Iter  3300 loss 54.532806 accuracy 0.18 lr 0.001000\n",
      "Iter  3400 loss 37.309448 accuracy 0.24 lr 0.001000\n",
      "Iter  3500 loss 45.302212 accuracy 0.14 lr 0.001000\n",
      "Iter  3600 loss 49.159073 accuracy 0.18 lr 0.001000\n",
      "Iter  3700 loss 42.398384 accuracy 0.17 lr 0.001000\n",
      "Iter  3800 loss 46.715385 accuracy 0.21 lr 0.001000\n",
      "Iter  3900 loss 38.903053 accuracy 0.16 lr 0.001000\n",
      "Iter  4000 loss 24.927929 accuracy 0.24 lr 0.001000\n",
      "Iter  4100 loss 57.705956 accuracy 0.21 lr 0.001000\n",
      "Iter  4200 loss 41.086155 accuracy 0.17 lr 0.001000\n",
      "Iter  4300 loss 48.671738 accuracy 0.20 lr 0.001000\n",
      "Iter  4400 loss 48.633095 accuracy 0.16 lr 0.001000\n",
      "Iter  4500 loss 39.258392 accuracy 0.14 lr 0.001000\n",
      "Iter  4600 loss 49.693466 accuracy 0.18 lr 0.001000\n",
      "Iter  4700 loss 33.492771 accuracy 0.24 lr 0.001000\n",
      "Iter  4800 loss 44.995003 accuracy 0.18 lr 0.001000\n",
      "Iter  4900 loss 44.785301 accuracy 0.20 lr 0.001000\n",
      "Iter  5000 loss 45.797642 accuracy 0.20 lr 0.001000\n",
      "Iter  5100 loss 44.507549 accuracy 0.18 lr 0.001000\n",
      "Iter  5200 loss 45.567841 accuracy 0.18 lr 0.001000\n",
      "Iter  5300 loss 46.540134 accuracy 0.17 lr 0.001000\n",
      "Iter  5400 loss 43.861156 accuracy 0.25 lr 0.001000\n",
      "Iter  5500 loss 39.936054 accuracy 0.21 lr 0.001000\n",
      "Iter  5600 loss 64.886185 accuracy 0.19 lr 0.001000\n",
      "Iter  5700 loss 46.306427 accuracy 0.24 lr 0.001000\n",
      "Iter  5800 loss 46.706497 accuracy 0.18 lr 0.001000\n",
      "Iter  5900 loss 47.448299 accuracy 0.18 lr 0.001000\n",
      "Iter  6000 loss 48.381622 accuracy 0.17 lr 0.001000\n",
      "Iter  6100 loss 45.669342 accuracy 0.19 lr 0.001000\n",
      "Iter  6200 loss 48.834312 accuracy 0.21 lr 0.001000\n",
      "Iter  6300 loss 71.192627 accuracy 0.13 lr 0.001000\n",
      "Iter  6400 loss 44.300240 accuracy 0.15 lr 0.001000\n",
      "Iter  6500 loss 36.425774 accuracy 0.17 lr 0.001000\n",
      "Iter  6600 loss 44.232403 accuracy 0.22 lr 0.001000\n",
      "Iter  6700 loss 52.940613 accuracy 0.17 lr 0.001000\n",
      "Iter  6800 loss 47.665009 accuracy 0.16 lr 0.001000\n",
      "Iter  6900 loss 42.631195 accuracy 0.22 lr 0.001000\n",
      "Iter  7000 loss 43.481747 accuracy 0.16 lr 0.001000\n",
      "Iter  7100 loss 50.709648 accuracy 0.24 lr 0.001000\n",
      "Iter  7200 loss 42.901958 accuracy 0.22 lr 0.001000\n",
      "Iter  7300 loss 66.300491 accuracy 0.19 lr 0.001000\n",
      "Iter  7400 loss 49.392010 accuracy 0.20 lr 0.001000\n",
      "Iter  7500 loss 39.426598 accuracy 0.22 lr 0.001000\n",
      "Iter  7600 loss 29.601620 accuracy 0.26 lr 0.001000\n",
      "Iter  7700 loss 51.684326 accuracy 0.21 lr 0.001000\n",
      "Iter  7800 loss 50.664242 accuracy 0.19 lr 0.001000\n",
      "Iter  7900 loss 44.228241 accuracy 0.23 lr 0.001000\n",
      "Iter  8000 loss 51.425644 accuracy 0.20 lr 0.001000\n",
      "Iter  8100 loss 37.759850 accuracy 0.19 lr 0.001000\n",
      "Iter  8200 loss 44.004017 accuracy 0.18 lr 0.001000\n",
      "Iter  8300 loss 37.572269 accuracy 0.21 lr 0.001000\n",
      "Iter  8400 loss 54.880306 accuracy 0.22 lr 0.001000\n",
      "Iter  8500 loss 46.549278 accuracy 0.23 lr 0.001000\n",
      "Iter  8600 loss 40.611160 accuracy 0.15 lr 0.001000\n",
      "Iter  8700 loss 30.459755 accuracy 0.22 lr 0.001000\n",
      "Iter  8800 loss 44.096157 accuracy 0.20 lr 0.001000\n",
      "Iter  8900 loss 49.700890 accuracy 0.22 lr 0.001000\n",
      "Iter  9000 loss 37.268970 accuracy 0.19 lr 0.001000\n",
      "Iter  9100 loss 44.112137 accuracy 0.20 lr 0.001000\n",
      "Iter  9200 loss 43.805027 accuracy 0.20 lr 0.001000\n",
      "Iter  9300 loss 49.482590 accuracy 0.20 lr 0.001000\n",
      "Iter  9400 loss 49.719109 accuracy 0.16 lr 0.001000\n",
      "Iter  9500 loss 56.818321 accuracy 0.15 lr 0.001000\n",
      "Iter  9600 loss 40.052517 accuracy 0.21 lr 0.001000\n",
      "Iter  9700 loss 38.836159 accuracy 0.27 lr 0.001000\n",
      "Iter  9800 loss 46.639709 accuracy 0.23 lr 0.001000\n",
      "Iter  9900 loss 28.781694 accuracy 0.19 lr 0.001000\n",
      "Iter 10000 loss 51.923042 accuracy 0.17 lr 0.001000\n",
      "Iter 10100 loss 29.981216 accuracy 0.28 lr 0.001000\n",
      "Iter 10200 loss 50.009102 accuracy 0.23 lr 0.001000\n",
      "Iter 10300 loss 29.856113 accuracy 0.22 lr 0.001000\n",
      "Iter 10400 loss 41.238495 accuracy 0.21 lr 0.001000\n",
      "Iter 10500 loss 52.147041 accuracy 0.18 lr 0.001000\n",
      "Iter 10600 loss 52.170792 accuracy 0.19 lr 0.001000\n",
      "Iter 10700 loss 43.713478 accuracy 0.20 lr 0.001000\n",
      "Iter 10800 loss 49.326683 accuracy 0.21 lr 0.001000\n",
      "Iter 10900 loss 46.918392 accuracy 0.21 lr 0.001000\n",
      "Iter 11000 loss 38.532757 accuracy 0.16 lr 0.001000\n",
      "Iter 11100 loss 44.950829 accuracy 0.21 lr 0.001000\n",
      "Epoch 2 loss 44.763471 accuracy 0.20 val_aer 0.92 val_acc 0.18\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 45.778027 accuracy 0.21 lr 0.001000\n",
      "Iter   200 loss 40.886734 accuracy 0.19 lr 0.001000\n",
      "Iter   300 loss 46.966179 accuracy 0.21 lr 0.001000\n",
      "Iter   400 loss 55.813362 accuracy 0.17 lr 0.001000\n",
      "Iter   500 loss 42.806210 accuracy 0.21 lr 0.001000\n",
      "Iter   600 loss 53.451820 accuracy 0.20 lr 0.001000\n",
      "Iter   700 loss 56.606640 accuracy 0.15 lr 0.001000\n",
      "Iter   800 loss 41.126282 accuracy 0.23 lr 0.001000\n",
      "Iter   900 loss 43.642185 accuracy 0.22 lr 0.001000\n",
      "Iter  1000 loss 36.528412 accuracy 0.24 lr 0.001000\n",
      "Iter  1100 loss 42.725277 accuracy 0.16 lr 0.001000\n",
      "Iter  1200 loss 35.109985 accuracy 0.18 lr 0.001000\n",
      "Iter  1300 loss 41.997246 accuracy 0.20 lr 0.001000\n",
      "Iter  1400 loss 54.277729 accuracy 0.14 lr 0.001000\n",
      "Iter  1500 loss 37.937874 accuracy 0.18 lr 0.001000\n",
      "Iter  1600 loss 45.800957 accuracy 0.20 lr 0.001000\n",
      "Iter  1700 loss 26.371662 accuracy 0.23 lr 0.001000\n",
      "Iter  1800 loss 34.784863 accuracy 0.22 lr 0.001000\n",
      "Iter  1900 loss 33.427929 accuracy 0.22 lr 0.001000\n",
      "Iter  2000 loss 40.292225 accuracy 0.21 lr 0.001000\n",
      "Iter  2100 loss 43.625851 accuracy 0.21 lr 0.001000\n",
      "Iter  2200 loss 51.042881 accuracy 0.23 lr 0.001000\n",
      "Iter  2300 loss 35.933113 accuracy 0.17 lr 0.001000\n",
      "Iter  2400 loss 39.546978 accuracy 0.21 lr 0.001000\n",
      "Iter  2500 loss 37.338856 accuracy 0.24 lr 0.001000\n",
      "Iter  2600 loss 51.660316 accuracy 0.16 lr 0.001000\n",
      "Iter  2700 loss 37.318718 accuracy 0.18 lr 0.001000\n",
      "Iter  2800 loss 44.222504 accuracy 0.19 lr 0.001000\n",
      "Iter  2900 loss 41.218742 accuracy 0.22 lr 0.001000\n",
      "Iter  3000 loss 35.811142 accuracy 0.16 lr 0.001000\n",
      "Iter  3100 loss 41.023567 accuracy 0.24 lr 0.001000\n",
      "Iter  3200 loss 53.530357 accuracy 0.19 lr 0.001000\n",
      "Iter  3300 loss 53.958530 accuracy 0.21 lr 0.001000\n",
      "Iter  3400 loss 43.727707 accuracy 0.22 lr 0.001000\n",
      "Iter  3500 loss 49.183296 accuracy 0.16 lr 0.001000\n",
      "Iter  3600 loss 51.160637 accuracy 0.17 lr 0.001000\n",
      "Iter  3700 loss 35.759689 accuracy 0.17 lr 0.001000\n",
      "Iter  3800 loss 18.172184 accuracy 0.24 lr 0.001000\n",
      "Iter  3900 loss 45.135044 accuracy 0.19 lr 0.001000\n",
      "Iter  4000 loss 40.486885 accuracy 0.23 lr 0.001000\n",
      "Iter  4100 loss 45.184258 accuracy 0.16 lr 0.001000\n",
      "Iter  4200 loss 27.784763 accuracy 0.23 lr 0.001000\n",
      "Iter  4300 loss 43.724503 accuracy 0.17 lr 0.001000\n",
      "Iter  4400 loss 46.891769 accuracy 0.24 lr 0.001000\n",
      "Iter  4500 loss 36.643600 accuracy 0.21 lr 0.001000\n",
      "Iter  4600 loss 42.344967 accuracy 0.20 lr 0.001000\n",
      "Iter  4700 loss 46.923424 accuracy 0.18 lr 0.001000\n",
      "Iter  4800 loss 58.817421 accuracy 0.15 lr 0.001000\n",
      "Iter  4900 loss 41.768009 accuracy 0.20 lr 0.001000\n",
      "Iter  5000 loss 59.164040 accuracy 0.16 lr 0.001000\n",
      "Iter  5100 loss 39.741867 accuracy 0.21 lr 0.001000\n",
      "Iter  5200 loss 47.572067 accuracy 0.19 lr 0.001000\n",
      "Iter  5300 loss 39.750092 accuracy 0.21 lr 0.001000\n",
      "Iter  5400 loss 63.289391 accuracy 0.17 lr 0.001000\n",
      "Iter  5500 loss 57.838829 accuracy 0.17 lr 0.001000\n",
      "Iter  5600 loss 33.614449 accuracy 0.21 lr 0.001000\n",
      "Iter  5700 loss 46.203892 accuracy 0.18 lr 0.001000\n",
      "Iter  5800 loss 42.426163 accuracy 0.16 lr 0.001000\n",
      "Iter  5900 loss 44.265312 accuracy 0.15 lr 0.001000\n",
      "Iter  6000 loss 46.496674 accuracy 0.21 lr 0.001000\n",
      "Iter  6100 loss 39.624866 accuracy 0.19 lr 0.001000\n",
      "Iter  6200 loss 46.455616 accuracy 0.25 lr 0.001000\n",
      "Iter  6300 loss 52.046997 accuracy 0.18 lr 0.001000\n",
      "Iter  6400 loss 36.028034 accuracy 0.22 lr 0.001000\n",
      "Iter  6500 loss 32.908508 accuracy 0.19 lr 0.001000\n",
      "Iter  6600 loss 52.223732 accuracy 0.15 lr 0.001000\n",
      "Iter  6700 loss 35.891312 accuracy 0.21 lr 0.001000\n",
      "Iter  6800 loss 31.512472 accuracy 0.17 lr 0.001000\n",
      "Iter  6900 loss 47.159973 accuracy 0.18 lr 0.001000\n",
      "Iter  7000 loss 40.985317 accuracy 0.22 lr 0.001000\n",
      "Iter  7100 loss 47.959061 accuracy 0.20 lr 0.001000\n",
      "Iter  7200 loss 35.959263 accuracy 0.22 lr 0.001000\n",
      "Iter  7300 loss 54.367073 accuracy 0.17 lr 0.001000\n",
      "Iter  7400 loss 53.652344 accuracy 0.20 lr 0.001000\n",
      "Iter  7500 loss 66.409920 accuracy 0.19 lr 0.001000\n",
      "Iter  7600 loss 45.952396 accuracy 0.18 lr 0.001000\n",
      "Iter  7700 loss 57.276123 accuracy 0.19 lr 0.001000\n",
      "Iter  7800 loss 45.061485 accuracy 0.19 lr 0.001000\n",
      "Iter  7900 loss 36.436806 accuracy 0.17 lr 0.001000\n",
      "Iter  8000 loss 35.160191 accuracy 0.24 lr 0.001000\n",
      "Iter  8100 loss 37.974754 accuracy 0.23 lr 0.001000\n",
      "Iter  8200 loss 36.194813 accuracy 0.26 lr 0.001000\n",
      "Iter  8300 loss 63.698303 accuracy 0.16 lr 0.001000\n",
      "Iter  8400 loss 45.204788 accuracy 0.23 lr 0.001000\n",
      "Iter  8500 loss 53.696590 accuracy 0.20 lr 0.001000\n",
      "Iter  8600 loss 52.243401 accuracy 0.18 lr 0.001000\n",
      "Iter  8700 loss 41.959431 accuracy 0.18 lr 0.001000\n",
      "Iter  8800 loss 58.837925 accuracy 0.22 lr 0.001000\n",
      "Iter  8900 loss 55.010818 accuracy 0.11 lr 0.001000\n",
      "Iter  9000 loss 51.774376 accuracy 0.21 lr 0.001000\n",
      "Iter  9100 loss 34.575867 accuracy 0.20 lr 0.001000\n",
      "Iter  9200 loss 44.363705 accuracy 0.18 lr 0.001000\n",
      "Iter  9300 loss 38.911819 accuracy 0.19 lr 0.001000\n",
      "Iter  9400 loss 38.579395 accuracy 0.17 lr 0.001000\n",
      "Iter  9500 loss 37.112320 accuracy 0.23 lr 0.001000\n",
      "Iter  9600 loss 41.450954 accuracy 0.19 lr 0.001000\n",
      "Iter  9700 loss 27.710453 accuracy 0.19 lr 0.001000\n",
      "Iter  9800 loss 42.559738 accuracy 0.17 lr 0.001000\n",
      "Iter  9900 loss 35.424019 accuracy 0.20 lr 0.001000\n",
      "Iter 10000 loss 52.177963 accuracy 0.19 lr 0.001000\n",
      "Iter 10100 loss 38.716961 accuracy 0.22 lr 0.001000\n",
      "Iter 10200 loss 38.817917 accuracy 0.23 lr 0.001000\n",
      "Iter 10300 loss 50.640503 accuracy 0.23 lr 0.001000\n",
      "Iter 10400 loss 32.336800 accuracy 0.16 lr 0.001000\n",
      "Iter 10500 loss 56.098419 accuracy 0.18 lr 0.001000\n",
      "Iter 10600 loss 32.677872 accuracy 0.25 lr 0.001000\n",
      "Iter 10700 loss 60.847466 accuracy 0.23 lr 0.001000\n",
      "Iter 10800 loss 52.453499 accuracy 0.18 lr 0.001000\n",
      "Iter 10900 loss 39.702843 accuracy 0.15 lr 0.001000\n",
      "Iter 11000 loss 31.622032 accuracy 0.27 lr 0.001000\n",
      "Iter 11100 loss 45.609299 accuracy 0.23 lr 0.001000\n",
      "Epoch 3 loss 44.613726 accuracy 0.20 val_aer 0.94 val_acc 0.18\n",
      "Computing training-set likelihood\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a710a71adca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# now we can start training!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training started..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mdev_AERs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_AERs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_likelihoods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_likelihoods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rezka/Documents/College/Natural Language Processing 2/project_neuralibm/neuralibm1trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0;31m# evaluate training-set likelihoods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0mtrain_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m       \u001b[0mtrain_likelihoods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_likelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0;31m# evaluate dev-set likelihoods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rezka/Documents/College/Natural Language Processing 2/project_neuralibm/neuralibm1trainer.py\u001b[0m in \u001b[0;36mlikelihood\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    203\u001b[0m       }\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rezka/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rezka/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rezka/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/rezka/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rezka/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # some hyper-parameters\n",
    "    # tweak them as you wish\n",
    "    batch_size=16  # on CPU, use something much smaller e.g. 1-16\n",
    "    max_length=30\n",
    "    lr = 0.001\n",
    "    lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "    emb_dim = 64\n",
    "    mlp_dim = 128\n",
    "\n",
    "    # our model\n",
    "    # change context to : \"gate\", \"concat\", or \"col_discrete\". \"col_discrete is for T3\"\n",
    "    model = NeuralIBM1Model(\n",
    "        x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f, \n",
    "        batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess, context=\"col_discrete\")\n",
    "\n",
    "    # our trainer\n",
    "    trainer = NeuralIBM1Trainer(\n",
    "        model, train_e_path, train_f_path, \n",
    "        dev_e_path, dev_f_path, dev_wa,\n",
    "        test_e_path, test_f_path, test_wa,\n",
    "        num_epochs=10, batch_size=batch_size, \n",
    "        max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess)\n",
    "\n",
    "    # now first TF needs to initialize all the variables\n",
    "    print(\"Initializing variables..\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # now we can start training!\n",
    "    print(\"Training started..\")\n",
    "    results = trainer.train()\n",
    "    dev_AERs, test_AERs, train_likelihoods, dev_likelihoods = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_AERs)+1), dev_AERs, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "ax2 = plt.plot(range(1, len(test_AERs)+1), test_AERs, label='test-set')\n",
    "handles.extend(ax2)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AER')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(train_likelihoods)+1), train_likelihoods, label='training-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_likelihoods)+1), dev_likelihoods, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
