{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2: Neural IBM1 (with additional French context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first run a few imports:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from utils import smart_reader, bitext_reader\n",
    "from vocabulary import OrderedCounter, Vocabulary \n",
    "from utils import iterate_minibatches, prepare_data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the paths to our training and validation data, English side\n",
    "train_e_path = 'data/training/hansards.36.2.e.gz'\n",
    "train_f_path = 'data/training/hansards.36.2.f.gz'\n",
    "dev_e_path = 'data/validation/dev.e.gz'\n",
    "dev_f_path = 'data/validation/dev.f.gz'\n",
    "dev_wa = 'data/validation/dev.wa.nonullalign'\n",
    "test_e_path = 'data/test/test.e.gz'\n",
    "test_f_path = 'data/test/test.f.gz'\n",
    "test_wa = 'data/test/test.wa.nonullalign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using only 1000 words will result in many UNKs, but\n",
    "# it will make training a lot faster. \n",
    "# If you have a fast computer, a GPU, or a lot of time,\n",
    "# try with 10000 instead.\n",
    "max_tokens=1000\n",
    "\n",
    "corpus_e = smart_reader(train_e_path)    \n",
    "vocabulary_e = Vocabulary(corpus=corpus_e, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_e, open(\"vocabulary_e.pkl\", mode=\"wb\"))\n",
    "\n",
    "corpus_f = smart_reader(train_f_path)    \n",
    "vocabulary_f = Vocabulary(corpus=corpus_f, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_f, open(\"vocabulary_f.pkl\", mode=\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralibm1 import NeuralIBM1Model\n",
    "from neuralibm1trainer import NeuralIBM1Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with B=16 max_length=10 lr=0.001 lr_decay=0.0\n",
      "Initializing variables..\n",
      "Training started..\n",
      "Shuffling training data\n",
      "Iter   100 loss 20.404045 accuracy 0.19 lr 0.001000\n",
      "Iter   200 loss 24.604702 accuracy 0.23 lr 0.001000\n",
      "Iter   300 loss 17.281116 accuracy 0.27 lr 0.001000\n",
      "Iter   400 loss 17.743221 accuracy 0.27 lr 0.001000\n",
      "Iter   500 loss 13.900187 accuracy 0.24 lr 0.001000\n",
      "Iter   600 loss 14.868587 accuracy 0.34 lr 0.001000\n",
      "Iter   700 loss 17.562687 accuracy 0.26 lr 0.001000\n",
      "Iter   800 loss 14.545536 accuracy 0.29 lr 0.001000\n",
      "Iter   900 loss 14.369592 accuracy 0.34 lr 0.001000\n",
      "Iter  1000 loss 18.464468 accuracy 0.24 lr 0.001000\n",
      "Iter  1100 loss 13.228703 accuracy 0.27 lr 0.001000\n",
      "Iter  1200 loss 13.670849 accuracy 0.22 lr 0.001000\n",
      "Iter  1300 loss 11.000366 accuracy 0.27 lr 0.001000\n",
      "Iter  1400 loss 9.732920 accuracy 0.28 lr 0.001000\n",
      "Iter  1500 loss 7.865861 accuracy 0.29 lr 0.001000\n",
      "Iter  1600 loss 8.820678 accuracy 0.33 lr 0.001000\n",
      "Iter  1700 loss 6.783144 accuracy 0.38 lr 0.001000\n",
      "Iter  1800 loss 9.524487 accuracy 0.29 lr 0.001000\n",
      "Iter  1900 loss 9.890211 accuracy 0.30 lr 0.001000\n",
      "Iter  2000 loss 10.758219 accuracy 0.34 lr 0.001000\n",
      "Iter  2100 loss 12.723186 accuracy 0.28 lr 0.001000\n",
      "Iter  2200 loss 13.449250 accuracy 0.22 lr 0.001000\n",
      "Iter  2300 loss 7.791817 accuracy 0.29 lr 0.001000\n",
      "Iter  2400 loss 10.936308 accuracy 0.32 lr 0.001000\n",
      "Iter  2500 loss 9.683731 accuracy 0.27 lr 0.001000\n",
      "Iter  2600 loss 12.273974 accuracy 0.24 lr 0.001000\n",
      "Iter  2700 loss 6.469186 accuracy 0.36 lr 0.001000\n",
      "Iter  2800 loss 11.939276 accuracy 0.29 lr 0.001000\n",
      "Iter  2900 loss 10.315115 accuracy 0.30 lr 0.001000\n",
      "Iter  3000 loss 7.151023 accuracy 0.30 lr 0.001000\n",
      "Iter  3100 loss 7.204148 accuracy 0.32 lr 0.001000\n",
      "Iter  3200 loss 9.829716 accuracy 0.30 lr 0.001000\n",
      "Iter  3300 loss 6.504731 accuracy 0.41 lr 0.001000\n",
      "Iter  3400 loss 6.852333 accuracy 0.29 lr 0.001000\n",
      "Iter  3500 loss 8.838640 accuracy 0.29 lr 0.001000\n",
      "Iter  3600 loss 10.985348 accuracy 0.33 lr 0.001000\n",
      "Iter  3700 loss 7.996293 accuracy 0.29 lr 0.001000\n",
      "Iter  3800 loss 8.642438 accuracy 0.31 lr 0.001000\n",
      "Iter  3900 loss 7.856509 accuracy 0.34 lr 0.001000\n",
      "Iter  4000 loss 9.046294 accuracy 0.22 lr 0.001000\n",
      "Iter  4100 loss 8.734138 accuracy 0.33 lr 0.001000\n",
      "Iter  4200 loss 9.107984 accuracy 0.22 lr 0.001000\n",
      "Iter  4300 loss 9.442317 accuracy 0.31 lr 0.001000\n",
      "Iter  4400 loss 8.829309 accuracy 0.32 lr 0.001000\n",
      "Iter  4500 loss 9.342954 accuracy 0.28 lr 0.001000\n",
      "Iter  4600 loss 4.943807 accuracy 0.41 lr 0.001000\n",
      "Iter  4700 loss 9.503710 accuracy 0.21 lr 0.001000\n",
      "Epoch 1 loss 10.939958 accuracy 0.29 val_aer 0.91 val_acc 0.19\n",
      "Computing training-set likelihood\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # some hyper-parameters\n",
    "    # tweak them as you wish\n",
    "    batch_size=16  # on CPU, use something much smaller e.g. 1-16\n",
    "    max_length=10\n",
    "    lr = 0.001\n",
    "    lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "    emb_dim = 64\n",
    "    mlp_dim = 128\n",
    "\n",
    "    # our model\n",
    "    # change context to : \"gate\", \"concat\", or \"col_discrete\". \"col_discrete is for T3\"\n",
    "    model = NeuralIBM1Model(\n",
    "        x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f, \n",
    "        batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess, context=\"concat\")\n",
    "\n",
    "    # our trainer\n",
    "    trainer = NeuralIBM1Trainer(\n",
    "        model, train_e_path, train_f_path, \n",
    "        dev_e_path, dev_f_path, dev_wa,\n",
    "        test_e_path, test_f_path, test_wa,\n",
    "        num_epochs=10, batch_size=batch_size, \n",
    "        max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess)\n",
    "\n",
    "    # now first TF needs to initialize all the variables\n",
    "    print(\"Initializing variables..\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # now we can start training!\n",
    "    print(\"Training started..\")\n",
    "    results = trainer.train()\n",
    "    dev_AERs, test_AERs, train_likelihoods, dev_likelihoods = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print (np.__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_AERs)+1), dev_AERs, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "ax2 = plt.plot(range(1, len(test_AERs)+1), test_AERs, label='test-set')\n",
    "handles.extend(ax2)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AER')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(train_likelihoods)+1), train_likelihoods, label='training-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_likelihoods)+1), dev_likelihoods, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # some hyper-parameters\n",
    "    # tweak them as you wish\n",
    "    batch_size=16  # on CPU, use something much smaller e.g. 1-16\n",
    "    max_length=30\n",
    "    lr = 0.001\n",
    "    lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "    emb_dim = 64\n",
    "    mlp_dim = 128\n",
    "\n",
    "    # our model\n",
    "    # change context to : \"gate\", \"concat\", or \"col_discrete\". \"col_discrete is for T3\"\n",
    "    model = NeuralIBM1Model(\n",
    "        x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f, \n",
    "        batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess, context=\"gate\")\n",
    "\n",
    "    # our trainer\n",
    "    trainer = NeuralIBM1Trainer(\n",
    "        model, train_e_path, train_f_path, \n",
    "        dev_e_path, dev_f_path, dev_wa,\n",
    "        test_e_path, test_f_path, test_wa,\n",
    "        num_epochs=10, batch_size=batch_size, \n",
    "        max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess)\n",
    "\n",
    "    # now first TF needs to initialize all the variables\n",
    "    print(\"Initializing variables..\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # now we can start training!\n",
    "    print(\"Training started..\")\n",
    "    results = trainer.train()\n",
    "    dev_AERs, test_AERs, train_likelihoods, dev_likelihoods = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_AERs)+1), dev_AERs, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "ax2 = plt.plot(range(1, len(test_AERs)+1), test_AERs, label='test-set')\n",
    "handles.extend(ax2)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AER')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(train_likelihoods)+1), train_likelihoods, label='training-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_likelihoods)+1), dev_likelihoods, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
