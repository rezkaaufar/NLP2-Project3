{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural IBM1\n",
    "\n",
    "NLP2 2016/2017 Project 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first run a few imports:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the paths to our training and validation data, English side\n",
    "train_e_path = 'data/training/hansards.36.2.e.gz'\n",
    "train_f_path = 'data/training/hansards.36.2.f.gz'\n",
    "dev_e_path = 'data/validation/dev.e.gz'\n",
    "dev_f_path = 'data/validation/dev.f.gz'\n",
    "dev_wa = 'data/validation/dev.wa.nonullalign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['36', 'th', 'Parliament', ',', '2', 'nd', 'Session'], ['36', 'e', 'Législature', ',', '2', 'ième', 'Session'])\n",
      "(['edited', 'HANSARD', '*', 'NUMBER', '1'], ['hansard', 'RÉVISÉ', '*', 'NUMÉRO', '1'])\n",
      "(['contents'], ['table', 'DES', 'MATIÈRES'])\n",
      "(['Tuesday', ',', 'October', '12', ',', '1999'], ['le', 'mardi', '12', 'octobre', '1999'])\n"
     ]
    }
   ],
   "source": [
    "# check utils.py if you want to see how smart_reader and bitext_reader work in detail\n",
    "from utils import smart_reader, bitext_reader\n",
    "\n",
    "    \n",
    "def bitext_reader_demo(src_path, trg_path):\n",
    "  \"\"\"Demo of the bitext reader.\"\"\"\n",
    " \n",
    "  # create a reader\n",
    "  src_reader = smart_reader(src_path)\n",
    "  trg_reader = smart_reader(trg_path)\n",
    "  bitext = bitext_reader(src_reader, trg_reader)\n",
    "\n",
    "  # to see that it really works, try this:\n",
    "  print(next(bitext))\n",
    "  print(next(bitext))\n",
    "  print(next(bitext))\n",
    "  print(next(bitext))  \n",
    "\n",
    "\n",
    "bitext_reader_demo(train_e_path, train_f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 178928 sentences with max_length = 30\n"
     ]
    }
   ],
   "source": [
    "# To see how many sentences are left if you filter by length, you can do this:\n",
    "\n",
    "def demo_number_filtered_sentence_pairs(src_path, trg_path):\n",
    "  src_reader = smart_reader(src_path)\n",
    "  trg_reader = smart_reader(trg_path)\n",
    "  max_length = 30\n",
    "  bitext = bitext_reader(src_reader, trg_reader, max_length=max_length)   \n",
    "  num_sentences = sum([1 for _ in bitext])\n",
    "  print(\"There are {} sentences with max_length = {}\".format(num_sentences, max_length))\n",
    "  \n",
    "  \n",
    "demo_number_filtered_sentence_pairs(train_e_path, train_f_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's create a vocabulary!\n",
    "\n",
    "We first define a class `Vocabulary` that helps us convert tokens (words) into numbers. This is useful later, because then we can e.g. index a word embedding table using the ID of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check vocabulary.py to see how the Vocabulary class is defined\n",
    "from vocabulary import OrderedCounter, Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our Vocabulary class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary size: 36640\n",
      "Trimmed vocabulary size: 1005\n",
      "The index of \"<PAD>\" is: 0\n",
      "The index of \"<UNK>\" is: 1\n",
      "The index of \"the\" is: 5\n",
      "The token with index 0 is: <PAD>\n",
      "The token with index 1 is: <UNK>\n",
      "The token with index 2 is: <S>\n",
      "The token with index 3 is: </S>\n",
      "The token with index 4 is: <NULL>\n",
      "The token with index 5 is: the\n",
      "The token with index 6 is: .\n",
      "The token with index 7 is: ,\n",
      "The token with index 8 is: of\n",
      "The token with index 9 is: to\n",
      "The index of \"!@!_not_in_vocab_!@!\" is: 1\n"
     ]
    }
   ],
   "source": [
    "def vocabulary_demo():\n",
    "\n",
    "  # We used up a few lines in the previous example, so we set up\n",
    "  # our data generator again.\n",
    "  corpus = smart_reader(train_e_path)    \n",
    "\n",
    "  # Let's create a vocabulary given our (tokenized) corpus\n",
    "  vocabulary = Vocabulary(corpus=corpus)\n",
    "  print(\"Original vocabulary size: {}\".format(len(vocabulary)))\n",
    "\n",
    "  # Now we only keep the highest-frequency words\n",
    "  vocabulary_size=1000\n",
    "  vocabulary.trim(vocabulary_size)\n",
    "  print(\"Trimmed vocabulary size: {}\".format(len(vocabulary)))\n",
    "\n",
    "  # Now we can get word indexes using v.get_word_id():\n",
    "  for t in [\"<PAD>\", \"<UNK>\", \"the\"]:\n",
    "    print(\"The index of \\\"{}\\\" is: {}\".format(t, vocabulary.get_token_id(t)))\n",
    "\n",
    "  # And the inverse too, using v.i2t:\n",
    "  for i in range(10):\n",
    "    print(\"The token with index {} is: {}\".format(i, vocabulary.get_token(i)))\n",
    "\n",
    "  # Now let's try to get a word ID for a word not in the vocabulary\n",
    "  # we should get 1 (so, <UNK>)\n",
    "  for t in [\"!@!_not_in_vocab_!@!\"]:\n",
    "    print(\"The index of \\\"{}\\\" is: {}\".format(t, vocabulary.get_token_id(t)))\n",
    "    \n",
    "    \n",
    "vocabulary_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the vocabularies that we use further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size: 1005\n",
      "French vocabulary size: 1005\n",
      "\n",
      "A few English words:\n",
      "Lib\n",
      "Atlantic\n",
      "that\n",
      "crime\n",
      "introduce\n",
      "\n",
      "A few French words:\n",
      "démocratie\n",
      "crise\n",
      "montant\n",
      "forme\n",
      "depuis\n"
     ]
    }
   ],
   "source": [
    "# Using only 1000 words will result in many UNKs, but\n",
    "# it will make training a lot faster. \n",
    "# If you have a fast computer, a GPU, or a lot of time,\n",
    "# try with 10000 instead.\n",
    "max_tokens=1000\n",
    "\n",
    "corpus_e = smart_reader(train_e_path)    \n",
    "vocabulary_e = Vocabulary(corpus=corpus_e, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_e, open(\"vocabulary_e.pkl\", mode=\"wb\"))\n",
    "print(\"English vocabulary size: {}\".format(len(vocabulary_e)))\n",
    "\n",
    "corpus_f = smart_reader(train_f_path)    \n",
    "vocabulary_f = Vocabulary(corpus=corpus_f, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_f, open(\"vocabulary_f.pkl\", mode=\"wb\"))\n",
    "print(\"French vocabulary size: {}\".format(len(vocabulary_f)))\n",
    "print()\n",
    "\n",
    "\n",
    "def sample_words(vocabulary, n=5):\n",
    "  \"\"\"Print a few words from the vocabulary.\"\"\"\n",
    "  for _ in range(n):\n",
    "    token_id = np.random.randint(0, len(vocabulary) - 1)\n",
    "    print(vocabulary.get_token(token_id))\n",
    "\n",
    "\n",
    "print(\"A few English words:\")\n",
    "sample_words(vocabulary_e, n=5)\n",
    "print()\n",
    "\n",
    "print(\"A few French words:\")\n",
    "sample_words(vocabulary_f, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Mini-batching\n",
    "\n",
    "With our vocabulary, we still need a method that converts a whole sentence to a sequence of IDs.\n",
    "And, to speed up training, we would like to get a so-called mini-batch at a time: multiple of such sequences together. So our function takes a corpus iterator and a vocabulary, and returns a mini-batch of shape [Batch, Time], where the first dimension indexes the sentences in the batch, and the second the time steps in each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import iterate_minibatches, prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch of data that we will train on, as tokens:\n",
      "[(['36', 'th', 'Parliament', ',', '2', 'nd', 'Session'],\n",
      "  ['36', 'e', 'Législature', ',', '2', 'ième', 'Session']),\n",
      " (['edited', 'HANSARD', '*', 'NUMBER', '1'],\n",
      "  ['hansard', 'RÉVISÉ', '*', 'NUMÉRO', '1']),\n",
      " (['contents'], ['table', 'DES', 'MATIÈRES']),\n",
      " (['Tuesday', ',', 'October', '12', ',', '1999'],\n",
      "  ['le', 'mardi', '12', 'octobre', '1999'])]\n",
      "\n",
      "These are our inputs (i.e. words replaced by IDs):\n",
      "[[  4   1 745 325   7 262   1   1]\n",
      " [  4   1   1  67   1 238   0   0]\n",
      " [  4   1   0   0   0   0   0   0]\n",
      " [  4   1   7 813 882   7 297   0]]\n",
      "\n",
      "These are the outputs (the foreign sentences):\n",
      "[[  1   1   1   7 254   1   1]\n",
      " [  1   1  62   1 250   0   0]\n",
      " [  1 463   1   0   0   0   0]\n",
      " [  6   1   1 840 295   0   0]]\n",
      "\n",
      "This is the batch of data that we will train on, as tokens:\n",
      "[(['opening',\n",
      "   'OF',\n",
      "   'THE',\n",
      "   'SECOND',\n",
      "   'SESSION',\n",
      "   'OF',\n",
      "   'THE',\n",
      "   '36',\n",
      "   'TH',\n",
      "   'PARLIAMENT'],\n",
      "  ['ouverture',\n",
      "   'DE',\n",
      "   'LA',\n",
      "   'DEUXIÈME',\n",
      "   'SESSION',\n",
      "   'DE',\n",
      "   'LA',\n",
      "   '36E',\n",
      "   'LÉGISLATURE']),\n",
      " (['oaths', 'OF', 'OFFICE'], ['les', 'SERMENTS', 'De', 'OFFICE']),\n",
      " (['bill', 'C', '-', '1', '.'], ['projet', 'de', 'loi', 'C', '-', '1', '.']),\n",
      " (['introduction', 'and', 'first', 'reading'],\n",
      "  ['présentation', 'et', 'première', 'lecture'])]\n",
      "\n",
      "These are our inputs (i.e. words replaced by IDs):\n",
      "[[  4   1 488 800   1   1 488 800   1   1   1]\n",
      " [  4   1 488   1   0   0   0   0   0   0   0]\n",
      " [  4  63  99  20 238   6   0   0   0   0   0]\n",
      " [  4 931  10 125 385   0   0   0   0   0   0]]\n",
      "\n",
      "These are the outputs (the foreign sentences):\n",
      "[[  1 239 542   1   1 239 542   1   1]\n",
      " [  9   1   1   1   0   0   0   0   0]\n",
      " [ 40   5  34  94  14 250   8   0   0]\n",
      " [832  13 227 336   0   0   0   0   0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_reader = smart_reader(train_e_path)\n",
    "trg_reader = smart_reader(train_f_path)\n",
    "bitext = bitext_reader(src_reader, trg_reader)\n",
    "\n",
    "\n",
    "for batch_id, batch in enumerate(iterate_minibatches(bitext, batch_size=4)):\n",
    "\n",
    "  print(\"This is the batch of data that we will train on, as tokens:\")\n",
    "  pprint(batch)\n",
    "  print()\n",
    "\n",
    "  x, y = prepare_data(batch, vocabulary_e, vocabulary_f)\n",
    "\n",
    "  print(\"These are our inputs (i.e. words replaced by IDs):\")\n",
    "  print(x)\n",
    "  print()\n",
    "  \n",
    "  print(\"These are the outputs (the foreign sentences):\")\n",
    "  print(y)\n",
    "  print()\n",
    "\n",
    "  if batch_id > 0:\n",
    "    break  # stop after the first batch, this is just a demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4   1 488 800   1   1 488 800   1   1   1]\n",
      " [  4   1 488   1   0   0   0   0   0   0   0]\n",
      " [  4  63  99  20 238   6   0   0   0   0   0]\n",
      " [  4 931  10 125 385   0   0   0   0   0   0]]\n",
      "[[  0   1 239 542   1   1 239 542   1]\n",
      " [  0   9   1   1   1   0   0   0   0]\n",
      " [  0  40   5  34  94  14 250   8   0]\n",
      " [  0 832  13 227 336   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "y_prev = np.roll(y,1,axis=1)\n",
    "y_prev[:,0] = 0\n",
    "print(x)\n",
    "print(y_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice the following:\n",
    "\n",
    "1. Every English sequence starts with a 4, the ID for < NULL \\>.\n",
    "2. The longest sequence in the batch contains no padding symbols. Any sequences shorter, however, will have padding zeros.\n",
    "\n",
    "With our input pipeline in place, now let's create a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check neuralibm1.py for the Model code\n",
    "from neuralibm1 import NeuralIBM1Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Now that we have a model, we need to train it. To do so we define a Trainer class that takes our model as an argument and trains it, keeping track of some important information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check neuralibm1trainer.py for the Trainer code\n",
    "from neuralibm1trainer import NeuralIBM1Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate a model and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with B=16 max_length=30 lr=0.001 lr_decay=0.0\n",
      "Initializing variables..\n",
      "Training started..\n",
      "Shuffling training data\n",
      "Iter   100 loss 103.748985 accuracy 0.17 lr 0.001000\n",
      "Iter   200 loss 91.830307 accuracy 0.21 lr 0.001000\n",
      "Iter   300 loss 79.120453 accuracy 0.18 lr 0.001000\n",
      "Iter   400 loss 69.310944 accuracy 0.15 lr 0.001000\n",
      "Iter   500 loss 79.302353 accuracy 0.20 lr 0.001000\n",
      "Iter   600 loss 65.848312 accuracy 0.20 lr 0.001000\n",
      "Iter   700 loss 79.484390 accuracy 0.22 lr 0.001000\n",
      "Iter   800 loss 83.071861 accuracy 0.17 lr 0.001000\n",
      "Iter   900 loss 73.689438 accuracy 0.18 lr 0.001000\n",
      "Iter  1000 loss 74.673393 accuracy 0.20 lr 0.001000\n",
      "Iter  1100 loss 97.384689 accuracy 0.20 lr 0.001000\n",
      "Iter  1200 loss 99.982727 accuracy 0.16 lr 0.001000\n",
      "Iter  1300 loss 82.941444 accuracy 0.21 lr 0.001000\n",
      "Iter  1400 loss 98.431129 accuracy 0.21 lr 0.001000\n",
      "Iter  1500 loss 89.053741 accuracy 0.19 lr 0.001000\n",
      "Iter  1600 loss 106.089447 accuracy 0.18 lr 0.001000\n",
      "Iter  1700 loss 81.681793 accuracy 0.18 lr 0.001000\n",
      "Iter  1800 loss 93.520432 accuracy 0.20 lr 0.001000\n",
      "Iter  1900 loss 62.742340 accuracy 0.17 lr 0.001000\n",
      "Iter  2000 loss 89.392593 accuracy 0.15 lr 0.001000\n",
      "Iter  2100 loss 60.563892 accuracy 0.13 lr 0.001000\n",
      "Iter  2200 loss 97.166130 accuracy 0.16 lr 0.001000\n",
      "Iter  2300 loss 76.972557 accuracy 0.10 lr 0.001000\n",
      "Iter  2400 loss 83.202248 accuracy 0.10 lr 0.001000\n",
      "Iter  2500 loss 101.801872 accuracy 0.14 lr 0.001000\n",
      "Iter  2600 loss 77.283821 accuracy 0.08 lr 0.001000\n",
      "Iter  2700 loss 89.700996 accuracy 0.10 lr 0.001000\n",
      "Iter  2800 loss 92.279114 accuracy 0.15 lr 0.001000\n",
      "Iter  2900 loss 70.495850 accuracy 0.09 lr 0.001000\n",
      "Iter  3000 loss 85.242653 accuracy 0.15 lr 0.001000\n",
      "Iter  3100 loss 95.062180 accuracy 0.10 lr 0.001000\n",
      "Iter  3200 loss 93.498062 accuracy 0.12 lr 0.001000\n",
      "Iter  3300 loss 80.105255 accuracy 0.12 lr 0.001000\n",
      "Iter  3400 loss 102.398438 accuracy 0.11 lr 0.001000\n",
      "Iter  3500 loss 92.034546 accuracy 0.10 lr 0.001000\n",
      "Iter  3600 loss 79.795715 accuracy 0.14 lr 0.001000\n",
      "Iter  3700 loss 100.175392 accuracy 0.18 lr 0.001000\n",
      "Iter  3800 loss 89.258636 accuracy 0.08 lr 0.001000\n",
      "Iter  3900 loss 92.404846 accuracy 0.11 lr 0.001000\n",
      "Iter  4000 loss 95.039566 accuracy 0.07 lr 0.001000\n",
      "Iter  4100 loss 91.436661 accuracy 0.14 lr 0.001000\n",
      "Iter  4200 loss 91.667648 accuracy 0.15 lr 0.001000\n",
      "Iter  4300 loss 92.694740 accuracy 0.11 lr 0.001000\n",
      "Iter  4400 loss 89.124069 accuracy 0.19 lr 0.001000\n",
      "Iter  4500 loss 67.810448 accuracy 0.13 lr 0.001000\n",
      "Iter  4600 loss 81.298920 accuracy 0.17 lr 0.001000\n",
      "Iter  4700 loss 77.389717 accuracy 0.18 lr 0.001000\n",
      "Iter  4800 loss 116.159607 accuracy 0.17 lr 0.001000\n",
      "Iter  4900 loss 72.656860 accuracy 0.15 lr 0.001000\n",
      "Iter  5000 loss 81.047150 accuracy 0.15 lr 0.001000\n",
      "Iter  5100 loss 81.362701 accuracy 0.19 lr 0.001000\n",
      "Iter  5200 loss 74.086815 accuracy 0.17 lr 0.001000\n",
      "Iter  5300 loss 72.854202 accuracy 0.18 lr 0.001000\n",
      "Iter  5400 loss 82.696533 accuracy 0.15 lr 0.001000\n",
      "Iter  5500 loss 96.444885 accuracy 0.22 lr 0.001000\n",
      "Iter  5600 loss 98.682587 accuracy 0.17 lr 0.001000\n",
      "Iter  5700 loss 72.894485 accuracy 0.15 lr 0.001000\n",
      "Iter  5800 loss 89.743813 accuracy 0.14 lr 0.001000\n",
      "Iter  5900 loss 71.113953 accuracy 0.23 lr 0.001000\n",
      "Iter  6000 loss 88.110199 accuracy 0.17 lr 0.001000\n",
      "Iter  6100 loss 97.153862 accuracy 0.22 lr 0.001000\n",
      "Iter  6200 loss 81.363640 accuracy 0.23 lr 0.001000\n",
      "Iter  6300 loss 96.822998 accuracy 0.18 lr 0.001000\n",
      "Iter  6400 loss 95.110535 accuracy 0.24 lr 0.001000\n",
      "Iter  6500 loss 75.196625 accuracy 0.15 lr 0.001000\n",
      "Iter  6600 loss 89.330399 accuracy 0.15 lr 0.001000\n",
      "Iter  6700 loss 77.705772 accuracy 0.15 lr 0.001000\n",
      "Iter  6800 loss 83.349945 accuracy 0.17 lr 0.001000\n",
      "Iter  6900 loss 86.737000 accuracy 0.23 lr 0.001000\n",
      "Iter  7000 loss 91.155365 accuracy 0.16 lr 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-065a2aa507e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;31m# now we can start training!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training started..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/rezka/Documents/College/Natural Language Processing 2/project_neuralibm/neuralibm1trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m         }\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rezka/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rezka/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rezka/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/rezka/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rezka/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "  # some hyper-parameters\n",
    "  # tweak them as you wish\n",
    "  batch_size=16  # on CPU, use something much smaller e.g. 1-16\n",
    "  max_length=30\n",
    "  lr = 0.001\n",
    "  lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "  emb_dim = 64\n",
    "  mlp_dim = 128\n",
    "  \n",
    "  # our model\n",
    "  model = NeuralIBM1Model(\n",
    "    x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f, \n",
    "    batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess, context=\"col_discrete\")\n",
    "  \n",
    "  # our trainer\n",
    "  trainer = NeuralIBM1Trainer(\n",
    "    model, train_e_path, train_f_path, \n",
    "    dev_e_path, dev_f_path, dev_wa,\n",
    "    num_epochs=10, batch_size=batch_size, \n",
    "    max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess)\n",
    "\n",
    "  # now first TF needs to initialize all the variables\n",
    "  print(\"Initializing variables..\")\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  # now we can start training!\n",
    "  print(\"Training started..\")\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
