{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a VAE in TensorFlow\n",
    "\n",
    "\n",
    "Consider the following graphical model:\n",
    "\n",
    "![VAE](img/vae.png)\n",
    "\n",
    "where \n",
    "* $Z \\sim \\mathcal N(0, I)$ is a random embedding\n",
    "* $X \\sim \\mathrm{Cat}(f_\\theta(z))$ is a categorical distribution over a language's vocabulary\n",
    "* $f_\\theta(z) = \\mathrm{softmax(g_\\theta(z))}$ is a FFNN that predicts the parameters of our CPD\n",
    "\n",
    "Here is our joint distribution \n",
    "\\begin{align}\n",
    "p_\\theta(x, z) &= p_\\theta(z) P_\\theta(x|z) \\\\\n",
    " &= \\mathcal N(0, I) P_\\theta(x|z)\n",
    "\\end{align}\n",
    "\n",
    "Note that the marginal likelihood is intractable\n",
    "\n",
    "\\begin{align}\n",
    "    P_\\theta(x) &=  \\int p_\\theta(z,x) \\mathrm{d}z \\\\\n",
    "    &= p_\\theta(z)P_\\theta(x|z) \\\\\n",
    "\\end{align}\n",
    "\n",
    "because of the marginalisation over all possible random embeddings and this makes our posterior intractable too.\n",
    "\n",
    "## Training\n",
    "\n",
    "We will use variational inference to circumvent the intractable marginalisation, where we propose a variational approximation (a.k.a. *inference network*) $q_\\phi(z|x)$ with its own parameters $\\phi$.\n",
    "Since $Z$ is Gaussian-distributed, we choose $q_\\phi(z|x) = \\mathcal N(\\mu_\\phi(x), \\sigma^2_\\phi(x))$, where\n",
    "\n",
    "* $\\mu_\\phi(x) = u_\\phi(x)$\n",
    "* $\\sigma^2_\\phi(x) = \\exp(s_\\phi(x))$\n",
    "\n",
    "are FFNNs that locally predict an approximation to the true posterior mean and variance for each observation $x$.\n",
    "\n",
    "Our variational auto-encoder then boils down to:\n",
    "\n",
    "* an *inference network*, i.e., a neural network that \n",
    "    * reads in words\n",
    "    * embeds them\n",
    "    * for each word: \n",
    "        * predicts a vector of means $\\mu_\\phi(x)$\n",
    "        * predicts a vector of (log) variances $\\sigma_\\phi^2(x)$\n",
    "        * samples a random embedding by sampling $\\epsilon \\sim \\mathcal N(0, I)$ and returning $\\mu_\\phi(x) + \\epsilon \\sigma_\\phi(x)$\n",
    "\n",
    "* a *generative model*, i.e., a neural network that for each word position\n",
    "    * takes a sampled embedding $z$\n",
    "    * predicts the parameters of a categorical distribution over the vocabulary $f_\\theta(x)$\n",
    "    \n",
    "You will identify all these steps in the code.\n",
    "\n",
    "The model is trained to maximise a lowerbound on log-likelihood of training data, the ELBO:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal E_{\\mathcal D}(\\theta, \\phi) &= \\frac{1}{|D|} \\sum_{x_1^n \\in \\mathcal D} \\underbrace{\\sum_{i=1}^{n} \\underbrace{\\mathcal E(\\theta, \\phi|x_i)}_{\\text{word}}}_{\\text{sentence}}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathcal D$ is a set made of $|\\mathcal D|$ sentences, each of which is itself a sequence of words.\n",
    "The contribution to the ELBO due to each sentence is the sum of contributions from each word:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal E(\\theta, \\phi|x) &= \\mathbb E_{q_\\phi(Z|x)} \\left[ \\log P_\\theta(x|Z) \\right] - \\mathrm{KL}(q_\\phi(Z|x)||p_\\theta(z)) \\\\\n",
    " &= \\mathbb E_{\\epsilon \\sim \\mathcal N(0,I)} \\left[ \\log P_\\theta(x|Z=\\mu_\\phi(x) + \\epsilon \\sigma_\\phi(x)) \\right] - \\mathrm{KL}(q_\\phi(Z|x)||\\mathcal N(0, I)) \n",
    "\\end{align}\n",
    "\n",
    "which we usually approximate with a single sample $\\epsilon \\sim N(0, I)$ for each word $x$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal E(\\theta, \\phi|x) \n",
    " &\\approx \\mathbb \\log P_\\theta(x|Z=\\mu_\\phi(x) + \\epsilon \\sigma_\\phi(x)) - \\mathrm{KL}(q_\\phi(Z|x)||\\mathcal N(0, I)) \n",
    "\\end{align}\n",
    "\n",
    "and the KL term can be computed analytically\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{KL}(q(Z|x)||\\mathcal N(0, I)) &= -\\frac{1}{2} \\sum_{j=1}^d \\left( 1 + \\log \\sigma^2_{\\phi,j}(x) - \\mu^2_{\\phi,j}(x) - \\sigma^2_{\\phi,j}(x) \\right)\n",
    "\\end{align}\n",
    "\n",
    "where the summation is defined over the $d$ components of the mean and variance vectors.\n",
    "\n",
    "\n",
    "## Posterior Inference\n",
    "\n",
    "Note that in general, because the generative model involves non-linear functions of $Z$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb E_{p_\\theta(Z|X)}[ f(Z) ]  & \\neq f\\left(\\mathbb E_{p_\\theta(Z|X)}[Z] \\right)\n",
    "\\end{align}\n",
    "\n",
    "where $p(Z|X=x)$ is approximated by our variational distribution $q_\\phi(Z|X=x)$.\n",
    "\n",
    "This means that decoding the mean is not the same as the mean decoding for a certain decoder $f$.\n",
    "\n",
    "Nonetheless, we will make a simplifying assumption here and approximate $\\mathbb E_{p_\\theta(Z|X=x)}[Z]$ by the predicted mean $\\mu_\\phi(x)$.\n",
    "\n",
    "A more principled approach would sample a few times from the approximate posterior and use a stochastic decoder (e.g. MBR), but this is beyond the scope of project 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first run a few imports:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "  \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "from collections import Counter, OrderedDict\n",
    "from aer import read_naacl_alignments, AERSufficientStatistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first load some data\n",
    "\n",
    "We define a reader that returns one sentence at a time, without loading the whole data set into memory.\n",
    "This is done using the \"yield\" command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['36', 'th', 'Parliament', ',', '2', 'nd', 'Session']\n",
      "['edited', 'HANSARD', '*', 'NUMBER', '1']\n",
      "['contents']\n"
     ]
    }
   ],
   "source": [
    "from utils import smart_reader, filter_len\n",
    "\n",
    "\n",
    "def reader_test(path):\n",
    "  # corpus is now a generator that gives us a list of tokens (a sentence) \n",
    "  # everytime a function calls \"next\" on it\n",
    "  corpus = filter_len(smart_reader(train_en_path), max_length=10)\n",
    "\n",
    "  # to see that it really works, try this:\n",
    "  print(next(corpus))\n",
    "  print(next(corpus))\n",
    "  print(next(corpus))\n",
    "  \n",
    "  \n",
    "# the path to our training data, English side\n",
    "train_en_path = 'data/training/hansards.36.2.e.gz'\n",
    "\n",
    "# Let's try it:\n",
    "reader_test(train_en_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's create a vocabulary!\n",
    "\n",
    "We first define a class `Vocabulary` that helps us convert tokens (words) into numbers. This is useful later, because then we can e.g. index a word embedding table using the ID of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our Vocabulary class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary size: 36640\n",
      "Trimmed vocabulary size: 10005\n",
      "The index of \"*PAD*\" is: 1\n",
      "The index of \"<UNK>\" is: 1\n",
      "The index of \"the\" is: 5\n",
      "The token with index 0 is: <PAD>\n",
      "The token with index 1 is: <UNK>\n",
      "The token with index 2 is: <S>\n",
      "The token with index 3 is: </S>\n",
      "The token with index 4 is: <NULL>\n",
      "The token with index 5 is: the\n",
      "The token with index 6 is: .\n",
      "The token with index 7 is: ,\n",
      "The token with index 8 is: of\n",
      "The token with index 9 is: to\n",
      "The index of \"!@!_not_in_vocab_!@!\" is: 1\n"
     ]
    }
   ],
   "source": [
    "# We used up a few lines in the previous example, so we set up\n",
    "# our data generator again.\n",
    "corpus = smart_reader(train_en_path)    \n",
    "\n",
    "# Let's create a vocabulary given our (tokenized) corpus\n",
    "vocabulary = Vocabulary(corpus=corpus)\n",
    "print(\"Original vocabulary size: {}\".format(len(vocabulary)))\n",
    "\n",
    "# Now we only keep the highest-frequency words\n",
    "vocabulary_size=10000\n",
    "vocabulary.trim(vocabulary_size)\n",
    "print(\"Trimmed vocabulary size: {}\".format(len(vocabulary)))\n",
    "\n",
    "# Now we can get word indexes using v.get_word_id():\n",
    "for t in [\"*PAD*\", \"<UNK>\", \"the\"]:\n",
    "  print(\"The index of \\\"{}\\\" is: {}\".format(t, vocabulary.get_token_id(t)))\n",
    "\n",
    "# And the inverse too, using v.i2t:\n",
    "for i in range(10):\n",
    "  print(\"The token with index {} is: {}\".format(i, vocabulary.get_token(i)))\n",
    "\n",
    "# Now let's try to get a word ID for a word not in the vocabulary\n",
    "# we should get 1 (so, <UNK>)\n",
    "for t in [\"!@!_not_in_vocab_!@!\"]:\n",
    "  print(\"The index of \\\"{}\\\" is: {}\".format(t, vocabulary.get_token_id(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Mini-batching\n",
    "\n",
    "With our vocabulary, we still need a method that converts a whole sentence to a sequence of IDs.\n",
    "And, to speed up training, we would like to get a so-called mini-batch at a time: multiple of such sequences together. So our function takes a corpus iterator and a vocabulary, and returns a mini-batch of dimension Batch X Time, where the first dimension indeces the sentences in the batch, and the second the time steps in each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import iterate_minibatches, prepare_batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the batch of data that we will train on, as tokens:\n",
      "[['36', 'th', 'Parliament', ',', '2', 'nd', 'Session'], ['edited', 'HANSARD', '*', 'NUMBER', '1'], ['contents'], ['Tuesday', ',', 'October', '12', ',', '1999']]\n",
      "\n",
      "These are our inputs (i.e. words replaced by IDs):\n",
      "[[   4 1203  745  325    7  262 2381 1963]\n",
      " [   4 2651 2665   67 2643  238    0    0]\n",
      " [   4 2873    0    0    0    0    0    0]\n",
      " [   4 1532    7  813  882    7  297    0]]\n",
      "\n",
      "Here is the original first sentence back again:\n",
      "['<NULL>', '36', 'th', 'Parliament', ',', '2', 'nd', 'Session']\n"
     ]
    }
   ],
   "source": [
    "# Let's try it out!\n",
    "corpus = smart_reader(train_en_path)          \n",
    "\n",
    "\n",
    "for batch_id, batch in enumerate(iterate_minibatches(corpus, batch_size=4)):\n",
    "\n",
    "  print(\"This is the batch of data that we will train on, as tokens:\")\n",
    "  print(batch)\n",
    "  print()\n",
    "\n",
    "  x = prepare_batch_data(batch, vocabulary)\n",
    "\n",
    "  print(\"These are our inputs (i.e. words replaced by IDs):\")\n",
    "  print(x)\n",
    "  print()\n",
    "  \n",
    "  print(\"Here is the original first sentence back again:\")\n",
    "  print([vocabulary.get_token(token_id) for token_id in x[0]])\n",
    "\n",
    "  break  # stop after the first batch, this is just a demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice the following:\n",
    "\n",
    "1. The longest sequence in the batch has no padding. Any sequences shorter, however, will have padding zeros.\n",
    "2. The length tensor gives the length for each sequence in the batch, so that we can correctly calculate the loss.\n",
    "\n",
    "With our input pipeline in place, now let's create a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check vae.py to see the model\n",
    "from vae import VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Now that we have a model, we need to train it. To do so we define a Trainer class that takes our model as an argument and trains it, keeping track of some important information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing variables..\n",
      "Training started..\n",
      "Iter 100 loss 990.9293823242188 ce 395.82684326171875 kl 595.1025390625 acc 0.15 127/842 lr 0.001000\n",
      "Iter 200 loss 648.8838500976562 ce 358.5419921875 kl 290.34185791015625 acc 0.40 379/937 lr 0.001000\n",
      "Iter 300 loss 505.9349365234375 ce 317.25982666015625 kl 188.6751251220703 acc 0.49 457/942 lr 0.001000\n",
      "Iter 400 loss 363.85870361328125 ce 235.06683349609375 kl 128.7918701171875 acc 0.54 420/783 lr 0.001000\n",
      "Iter 500 loss 387.63946533203125 ce 266.2981872558594 kl 121.34127807617188 acc 0.55 492/889 lr 0.001000\n",
      "Iter 600 loss 369.08770751953125 ce 258.7518615722656 kl 110.33585357666016 acc 0.59 546/926 lr 0.001000\n",
      "Iter 700 loss 296.1138000488281 ce 205.559814453125 kl 90.55398559570312 acc 0.60 462/774 lr 0.001000\n",
      "Iter 800 loss 359.9495544433594 ce 254.54965209960938 kl 105.39990234375 acc 0.62 614/984 lr 0.001000\n",
      "Iter 900 loss 310.8501892089844 ce 218.02670288085938 kl 92.823486328125 acc 0.65 582/892 lr 0.001000\n",
      "Iter 1000 loss 644.7874755859375 ce 220.78427124023438 kl 424.003173828125 acc 0.57 501/885 lr 0.001000\n",
      "Iter 1100 loss 495.0119934082031 ce 189.20993041992188 kl 305.80206298828125 acc 0.60 510/847 lr 0.001000\n",
      "Iter 1200 loss 524.8887939453125 ce 221.623779296875 kl 303.2650451660156 acc 0.61 585/961 lr 0.001000\n",
      "Iter 1300 loss 438.73236083984375 ce 185.03024291992188 kl 253.70213317871094 acc 0.65 560/858 lr 0.001000\n",
      "Iter 1400 loss 473.68560791015625 ce 200.734619140625 kl 272.95098876953125 acc 0.65 636/980 lr 0.001000\n",
      "Iter 1500 loss 414.1438903808594 ce 176.51791381835938 kl 237.6259765625 acc 0.65 573/875 lr 0.001000\n",
      "Iter 1600 loss 384.94085693359375 ce 161.69171142578125 kl 223.2491455078125 acc 0.69 594/859 lr 0.001000\n",
      "Iter 1700 loss 399.06689453125 ce 165.234375 kl 233.83251953125 acc 0.71 649/915 lr 0.001000\n",
      "Iter 1800 loss 407.19537353515625 ce 181.7972412109375 kl 225.3981170654297 acc 0.68 623/917 lr 0.001000\n",
      "Iter 1900 loss 373.2196350097656 ce 153.7716522216797 kl 219.44798278808594 acc 0.72 657/910 lr 0.001000\n",
      "Iter 2000 loss 361.96380615234375 ce 152.19187927246094 kl 209.77191162109375 acc 0.73 661/910 lr 0.001000\n",
      "Iter 2100 loss 349.3197021484375 ce 151.8276824951172 kl 197.4920196533203 acc 0.69 599/862 lr 0.001000\n",
      "Iter 2200 loss 352.89111328125 ce 147.71963500976562 kl 205.17149353027344 acc 0.73 655/901 lr 0.001000\n",
      "Iter 2300 loss 343.907470703125 ce 147.9502716064453 kl 195.95721435546875 acc 0.72 644/896 lr 0.001000\n",
      "Iter 2400 loss 377.86749267578125 ce 161.7257843017578 kl 216.1417236328125 acc 0.75 758/1014 lr 0.001000\n",
      "Iter 2500 loss 326.2785949707031 ce 142.19509887695312 kl 184.08349609375 acc 0.72 615/860 lr 0.001000\n",
      "Iter 2600 loss 321.41839599609375 ce 139.86764526367188 kl 181.5507354736328 acc 0.73 624/855 lr 0.001000\n",
      "Iter 2700 loss 295.923828125 ce 123.25462341308594 kl 172.669189453125 acc 0.75 621/831 lr 0.001000\n",
      "Iter 2800 loss 337.22760009765625 ce 144.80857849121094 kl 192.41903686523438 acc 0.73 677/928 lr 0.001000\n",
      "Iter 2900 loss 309.2129211425781 ce 126.46450805664062 kl 182.7484130859375 acc 0.77 685/894 lr 0.001000\n",
      "Iter 3000 loss 343.85894775390625 ce 149.05020141601562 kl 194.80874633789062 acc 0.74 740/1003 lr 0.001000\n",
      "Epoch 1 epoch_loss 236326.23542629537\n",
      "Model saved in file: model.ckpt\n",
      "Iter 3179 loss 316.11151123046875 ce 136.91311645507812 kl 179.19837951660156 acc 0.75 697/924 lr 0.001000\n",
      "Iter 3279 loss 320.0487060546875 ce 132.40301513671875 kl 187.64569091796875 acc 0.77 750/974 lr 0.001000\n",
      "Iter 3379 loss 288.3458251953125 ce 120.51343536376953 kl 167.83238220214844 acc 0.76 681/893 lr 0.001000\n",
      "Iter 3479 loss 289.726318359375 ce 122.27606201171875 kl 167.4502410888672 acc 0.78 703/905 lr 0.001000\n",
      "Iter 3579 loss 295.6898193359375 ce 122.76914978027344 kl 172.92068481445312 acc 0.77 729/942 lr 0.001000\n",
      "Iter 3679 loss 273.1814270019531 ce 113.18234252929688 kl 159.99908447265625 acc 0.79 694/878 lr 0.001000\n",
      "Iter 3779 loss 284.4054870605469 ce 112.70364379882812 kl 171.70184326171875 acc 0.80 762/954 lr 0.001000\n",
      "Iter 3879 loss 330.136962890625 ce 142.5116424560547 kl 187.62533569335938 acc 0.75 795/1061 lr 0.001000\n",
      "Iter 3979 loss 267.5787048339844 ce 112.74359130859375 kl 154.83511352539062 acc 0.78 689/886 lr 0.001000\n",
      "Iter 4079 loss 239.67852783203125 ce 100.62704467773438 kl 139.05148315429688 acc 0.78 632/808 lr 0.001000\n",
      "Iter 4179 loss 252.15789794921875 ce 103.91669464111328 kl 148.24119567871094 acc 0.78 683/876 lr 0.001000\n",
      "Iter 4279 loss 285.5827941894531 ce 119.23878479003906 kl 166.34400939941406 acc 0.79 787/996 lr 0.001000\n",
      "Iter 4379 loss 255.76580810546875 ce 104.80394744873047 kl 150.96185302734375 acc 0.80 739/919 lr 0.001000\n",
      "Iter 4479 loss 223.56024169921875 ce 90.18568420410156 kl 133.3745574951172 acc 0.80 624/781 lr 0.001000\n",
      "Iter 4579 loss 246.5655975341797 ce 100.8248291015625 kl 145.7407684326172 acc 0.79 702/889 lr 0.001000\n",
      "Iter 4679 loss 227.83836364746094 ce 95.47607421875 kl 132.36228942871094 acc 0.80 653/820 lr 0.001000\n",
      "Iter 4779 loss 236.83924865722656 ce 97.63150024414062 kl 139.20774841308594 acc 0.84 729/870 lr 0.001000\n",
      "Iter 4879 loss 239.7666778564453 ce 102.35670471191406 kl 137.40997314453125 acc 0.82 726/889 lr 0.001000\n",
      "Iter 4979 loss 218.37393188476562 ce 90.866455078125 kl 127.50748443603516 acc 0.79 628/794 lr 0.001000\n",
      "Iter 5079 loss 229.5438995361328 ce 93.66079711914062 kl 135.8831024169922 acc 0.84 752/895 lr 0.001000\n",
      "Iter 5179 loss 222.00674438476562 ce 92.85051727294922 kl 129.15621948242188 acc 0.81 702/862 lr 0.001000\n",
      "Iter 5279 loss 232.55345153808594 ce 99.33319091796875 kl 133.2202606201172 acc 0.80 708/890 lr 0.001000\n",
      "Iter 5379 loss 216.41259765625 ce 88.37369537353516 kl 128.03890991210938 acc 0.82 706/858 lr 0.001000\n",
      "Iter 5479 loss 236.2552490234375 ce 100.41606140136719 kl 135.8391876220703 acc 0.82 770/942 lr 0.001000\n",
      "Iter 5579 loss 183.89019775390625 ce 74.95136260986328 kl 108.93882751464844 acc 0.82 608/740 lr 0.001000\n",
      "Iter 5679 loss 228.05828857421875 ce 96.24004364013672 kl 131.81825256347656 acc 0.80 740/921 lr 0.001000\n",
      "Iter 5779 loss 210.54425048828125 ce 86.7476806640625 kl 123.79657745361328 acc 0.85 748/885 lr 0.001000\n",
      "Iter 5879 loss 215.43951416015625 ce 88.24091339111328 kl 127.19860076904297 acc 0.83 766/922 lr 0.001000\n",
      "Iter 5979 loss 197.17596435546875 ce 79.64291381835938 kl 117.53305053710938 acc 0.84 698/829 lr 0.001000\n",
      "Iter 6079 loss 222.1048583984375 ce 91.97256469726562 kl 130.13229370117188 acc 0.83 775/937 lr 0.001000\n",
      "Epoch 2 epoch_loss 243.5467561459766\n",
      "Model saved in file: model.ckpt\n",
      "Iter 6258 loss 216.8729248046875 ce 92.27814483642578 kl 124.59477233886719 acc 0.79 701/884 lr 0.001000\n",
      "Iter 6358 loss 197.91310119628906 ce 82.31465148925781 kl 115.59844970703125 acc 0.80 665/831 lr 0.001000\n",
      "Iter 6458 loss 184.96035766601562 ce 75.31440734863281 kl 109.64595794677734 acc 0.82 670/815 lr 0.001000\n",
      "Iter 6558 loss 201.78976440429688 ce 85.89466094970703 kl 115.89510345458984 acc 0.84 734/873 lr 0.001000\n",
      "Iter 6658 loss 215.17276000976562 ce 87.96216583251953 kl 127.21058654785156 acc 0.82 794/967 lr 0.001000\n",
      "Iter 6758 loss 203.38168334960938 ce 82.76717376708984 kl 120.61450958251953 acc 0.84 773/922 lr 0.001000\n",
      "Iter 6858 loss 230.32562255859375 ce 99.3930435180664 kl 130.9325714111328 acc 0.82 847/1033 lr 0.001000\n",
      "Iter 6958 loss 185.41741943359375 ce 78.19790649414062 kl 107.21951293945312 acc 0.83 708/855 lr 0.001000\n",
      "Iter 7058 loss 198.63970947265625 ce 86.04328918457031 kl 112.59642791748047 acc 0.79 696/882 lr 0.001000\n",
      "Iter 7158 loss 202.1116485595703 ce 86.21387481689453 kl 115.89777374267578 acc 0.78 725/931 lr 0.001000\n",
      "Iter 7258 loss 182.42420959472656 ce 72.63288879394531 kl 109.79132080078125 acc 0.81 696/863 lr 0.001000\n",
      "Iter 7358 loss 161.6532745361328 ce 67.02056884765625 kl 94.63270568847656 acc 0.78 570/735 lr 0.001000\n",
      "Iter 7458 loss 178.32818603515625 ce 72.26774597167969 kl 106.06044006347656 acc 0.79 695/877 lr 0.001000\n",
      "Iter 7558 loss 179.98294067382812 ce 77.03215026855469 kl 102.9507827758789 acc 0.78 669/862 lr 0.001000\n",
      "Iter 7658 loss 193.71141052246094 ce 81.66014099121094 kl 112.05126953125 acc 0.76 728/953 lr 0.001000\n",
      "Iter 7758 loss 199.17388916015625 ce 84.94982147216797 kl 114.22407531738281 acc 0.77 746/975 lr 0.001000\n",
      "Iter 7858 loss 167.2322540283203 ce 68.55422973632812 kl 98.67802429199219 acc 0.78 653/834 lr 0.001000\n",
      "Iter 7958 loss 189.15478515625 ce 78.6927719116211 kl 110.46202087402344 acc 0.73 689/939 lr 0.001000\n",
      "Iter 8058 loss 181.66275024414062 ce 77.14075469970703 kl 104.5219955444336 acc 0.79 698/886 lr 0.001000\n",
      "Iter 8158 loss 181.3531951904297 ce 76.22624206542969 kl 105.126953125 acc 0.77 698/911 lr 0.001000\n",
      "Iter 8258 loss 169.99761962890625 ce 71.75597381591797 kl 98.24164581298828 acc 0.77 663/865 lr 0.001000\n",
      "Iter 8358 loss 166.05044555664062 ce 71.25788879394531 kl 94.79255676269531 acc 0.74 616/830 lr 0.001000\n",
      "Iter 8458 loss 171.96096801757812 ce 76.41029357910156 kl 95.55067443847656 acc 0.74 630/848 lr 0.001000\n",
      "Iter 8558 loss 159.8953094482422 ce 65.68406677246094 kl 94.21124267578125 acc 0.77 643/836 lr 0.001000\n",
      "Iter 8658 loss 166.56008911132812 ce 72.38461303710938 kl 94.17547607421875 acc 0.76 654/862 lr 0.001000\n",
      "Iter 8758 loss 179.85330200195312 ce 77.65103912353516 kl 102.20225524902344 acc 0.72 692/956 lr 0.001000\n",
      "Iter 8858 loss 169.48800659179688 ce 73.02401733398438 kl 96.46399688720703 acc 0.74 646/877 lr 0.001000\n",
      "Iter 8958 loss 204.14834594726562 ce 94.35717010498047 kl 109.79116821289062 acc 0.74 779/1046 lr 0.001000\n",
      "Iter 9058 loss 178.09898376464844 ce 78.76582336425781 kl 99.33316040039062 acc 0.75 705/938 lr 0.001000\n",
      "Iter 9158 loss 183.99789428710938 ce 80.40771484375 kl 103.5901870727539 acc 0.72 703/980 lr 0.001000\n",
      "Epoch 3 epoch_loss 181.63594142892757\n",
      "Model saved in file: model.ckpt\n",
      "Iter 9337 loss 165.72280883789062 ce 71.67709350585938 kl 94.04571533203125 acc 0.73 654/898 lr 0.001000\n",
      "Iter 9437 loss 167.4523162841797 ce 73.48876190185547 kl 93.96355438232422 acc 0.73 666/912 lr 0.001000\n",
      "Iter 9537 loss 169.51351928710938 ce 78.26631164550781 kl 91.2472152709961 acc 0.70 622/883 lr 0.001000\n",
      "Iter 9637 loss 193.224853515625 ce 86.66484069824219 kl 106.56001281738281 acc 0.73 804/1104 lr 0.001000\n",
      "Iter 9737 loss 154.65618896484375 ce 68.39706420898438 kl 86.25912475585938 acc 0.74 633/856 lr 0.001000\n",
      "Iter 9837 loss 170.954833984375 ce 75.98320007324219 kl 94.97164154052734 acc 0.75 728/977 lr 0.001000\n",
      "Iter 9937 loss 144.66522216796875 ce 60.63655090332031 kl 84.02867126464844 acc 0.73 605/829 lr 0.001000\n",
      "Iter 10037 loss 150.63543701171875 ce 68.87613677978516 kl 81.75930786132812 acc 0.72 599/836 lr 0.001000\n",
      "Iter 10137 loss 155.69488525390625 ce 69.62421417236328 kl 86.07067108154297 acc 0.73 656/898 lr 0.001000\n",
      "Iter 10237 loss 155.3490447998047 ce 69.38365936279297 kl 85.96538543701172 acc 0.71 637/893 lr 0.001000\n",
      "Iter 10337 loss 137.22430419921875 ce 62.287635803222656 kl 74.93667602539062 acc 0.72 572/791 lr 0.001000\n",
      "Iter 10437 loss 145.61012268066406 ce 66.53904724121094 kl 79.07107543945312 acc 0.70 576/827 lr 0.001000\n",
      "Iter 10537 loss 153.4529571533203 ce 68.00042724609375 kl 85.45252990722656 acc 0.72 664/917 lr 0.001000\n",
      "Iter 10637 loss 136.74424743652344 ce 62.67428970336914 kl 74.06996154785156 acc 0.75 605/809 lr 0.001000\n",
      "Iter 10737 loss 152.7444305419922 ce 69.43966674804688 kl 83.30476379394531 acc 0.70 637/914 lr 0.001000\n",
      "Iter 10837 loss 150.74594116210938 ce 69.75782775878906 kl 80.98812103271484 acc 0.67 589/876 lr 0.001000\n",
      "Iter 10937 loss 152.30490112304688 ce 70.169921875 kl 82.13497924804688 acc 0.70 636/915 lr 0.001000\n",
      "Iter 11037 loss 129.3579864501953 ce 56.80681228637695 kl 72.55117797851562 acc 0.68 538/787 lr 0.001000\n",
      "Iter 11137 loss 143.91448974609375 ce 66.65025329589844 kl 77.26422882080078 acc 0.67 582/866 lr 0.001000\n",
      "Iter 11237 loss 136.3428955078125 ce 61.06995391845703 kl 75.27293395996094 acc 0.70 585/837 lr 0.001000\n"
     ]
    }
   ],
   "source": [
    "from vae_trainer import VAETrainer\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#   with tf.device(\"/cpu:0\"):   \n",
    "\n",
    "  batch_size=64\n",
    "  max_length=30\n",
    "\n",
    "  model = VAE(vocabulary=vocabulary, batch_size=batch_size, \n",
    "              emb_dim=64, rnn_dim=128, z_dim=64)\n",
    "  trainer = VAETrainer(model, train_en_path, num_epochs=10, \n",
    "                  batch_size=batch_size, max_length=max_length,\n",
    "                  lr=0.001, lr_decay=0.0, session=sess)\n",
    "\n",
    "  print(\"Initializing variables..\")\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  print(\"Training started..\")\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}