{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2: Neural IBM1 (with additional French context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first run a few imports:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from utils import smart_reader, bitext_reader\n",
    "from vocabulary import OrderedCounter, Vocabulary \n",
    "from utils import iterate_minibatches, prepare_data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the paths to our training and validation data, English side\n",
    "train_e_path = 'data/training/hansards.36.2.e.gz'\n",
    "train_f_path = 'data/training/hansards.36.2.f.gz'\n",
    "dev_e_path = 'data/validation/dev.e.gz'\n",
    "dev_f_path = 'data/validation/dev.f.gz'\n",
    "dev_wa = 'data/validation/dev.wa.nonullalign'\n",
    "test_e_path = 'data/test/test.e.gz'\n",
    "test_f_path = 'data/test/test.f.gz'\n",
    "test_wa = 'data/test/test.wa.nonullalign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using only 1000 words will result in many UNKs, but\n",
    "# it will make training a lot faster. \n",
    "# If you have a fast computer, a GPU, or a lot of time,\n",
    "# try with 10000 instead.\n",
    "max_tokens=1000\n",
    "\n",
    "corpus_e = smart_reader(train_e_path)    \n",
    "vocabulary_e = Vocabulary(corpus=corpus_e, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_e, open(\"vocabulary_e.pkl\", mode=\"wb\"))\n",
    "\n",
    "corpus_f = smart_reader(train_f_path)    \n",
    "vocabulary_f = Vocabulary(corpus=corpus_f, max_tokens=max_tokens)\n",
    "pickle.dump(vocabulary_f, open(\"vocabulary_f.pkl\", mode=\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuralibm1 import NeuralIBM1Model\n",
    "from neuralibm1trainer import NeuralIBM1Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with B=16 max_length=30 lr=0.001 lr_decay=0.0\n",
      "Initializing variables..\n",
      "Training started..\n",
      "Shuffling training data\n",
      "Iter   100 loss 69.108368 accuracy 0.16 lr 0.001000\n",
      "Iter   200 loss 61.670876 accuracy 0.22 lr 0.001000\n",
      "Iter   300 loss 43.225018 accuracy 0.21 lr 0.001000\n",
      "Iter   400 loss 60.856987 accuracy 0.16 lr 0.001000\n",
      "Iter   500 loss 52.596558 accuracy 0.21 lr 0.001000\n",
      "Iter   600 loss 65.355202 accuracy 0.19 lr 0.001000\n",
      "Iter   700 loss 52.749512 accuracy 0.20 lr 0.001000\n",
      "Iter   800 loss 40.565758 accuracy 0.27 lr 0.001000\n",
      "Iter   900 loss 48.686516 accuracy 0.19 lr 0.001000\n",
      "Iter  1000 loss 49.435966 accuracy 0.24 lr 0.001000\n",
      "Iter  1100 loss 36.142769 accuracy 0.21 lr 0.001000\n",
      "Iter  1200 loss 51.674297 accuracy 0.20 lr 0.001000\n",
      "Iter  1300 loss 41.781189 accuracy 0.21 lr 0.001000\n",
      "Iter  1400 loss 59.232143 accuracy 0.17 lr 0.001000\n",
      "Iter  1500 loss 30.797688 accuracy 0.19 lr 0.001000\n",
      "Iter  1600 loss 41.330330 accuracy 0.21 lr 0.001000\n",
      "Iter  1700 loss 41.773239 accuracy 0.24 lr 0.001000\n",
      "Iter  1800 loss 39.672665 accuracy 0.21 lr 0.001000\n",
      "Iter  1900 loss 23.834055 accuracy 0.24 lr 0.001000\n",
      "Iter  2000 loss 28.905155 accuracy 0.19 lr 0.001000\n",
      "Iter  2100 loss 27.465017 accuracy 0.26 lr 0.001000\n",
      "Iter  2200 loss 35.642925 accuracy 0.20 lr 0.001000\n",
      "Iter  2300 loss 40.711178 accuracy 0.17 lr 0.001000\n",
      "Iter  2400 loss 32.574127 accuracy 0.16 lr 0.001000\n",
      "Iter  2500 loss 44.579147 accuracy 0.16 lr 0.001000\n",
      "Iter  2600 loss 39.644138 accuracy 0.19 lr 0.001000\n",
      "Iter  2700 loss 34.277359 accuracy 0.22 lr 0.001000\n",
      "Iter  2800 loss 40.982933 accuracy 0.26 lr 0.001000\n",
      "Iter  2900 loss 39.839245 accuracy 0.19 lr 0.001000\n",
      "Iter  3000 loss 27.825045 accuracy 0.21 lr 0.001000\n",
      "Iter  3100 loss 31.315163 accuracy 0.23 lr 0.001000\n",
      "Iter  3200 loss 38.096893 accuracy 0.20 lr 0.001000\n",
      "Iter  3300 loss 24.847404 accuracy 0.23 lr 0.001000\n",
      "Iter  3400 loss 36.258347 accuracy 0.23 lr 0.001000\n",
      "Iter  3500 loss 38.407722 accuracy 0.21 lr 0.001000\n",
      "Iter  3600 loss 32.407143 accuracy 0.18 lr 0.001000\n",
      "Iter  3700 loss 35.537155 accuracy 0.24 lr 0.001000\n",
      "Iter  3800 loss 34.238182 accuracy 0.19 lr 0.001000\n",
      "Iter  3900 loss 33.754841 accuracy 0.24 lr 0.001000\n",
      "Iter  4000 loss 31.939955 accuracy 0.19 lr 0.001000\n",
      "Iter  4100 loss 30.623005 accuracy 0.18 lr 0.001000\n",
      "Iter  4200 loss 42.457718 accuracy 0.21 lr 0.001000\n",
      "Iter  4300 loss 30.773733 accuracy 0.23 lr 0.001000\n",
      "Iter  4400 loss 27.503174 accuracy 0.26 lr 0.001000\n",
      "Iter  4500 loss 39.461449 accuracy 0.17 lr 0.001000\n",
      "Iter  4600 loss 28.051228 accuracy 0.25 lr 0.001000\n",
      "Iter  4700 loss 46.864815 accuracy 0.18 lr 0.001000\n",
      "Iter  4800 loss 41.956474 accuracy 0.18 lr 0.001000\n",
      "Iter  4900 loss 34.164467 accuracy 0.20 lr 0.001000\n",
      "Iter  5000 loss 40.163925 accuracy 0.24 lr 0.001000\n",
      "Iter  5100 loss 28.759998 accuracy 0.13 lr 0.001000\n",
      "Iter  5200 loss 40.736412 accuracy 0.18 lr 0.001000\n",
      "Iter  5300 loss 40.973541 accuracy 0.18 lr 0.001000\n",
      "Iter  5400 loss 44.505554 accuracy 0.22 lr 0.001000\n",
      "Iter  5500 loss 26.556501 accuracy 0.19 lr 0.001000\n",
      "Iter  5600 loss 27.355917 accuracy 0.27 lr 0.001000\n",
      "Iter  5700 loss 22.684576 accuracy 0.23 lr 0.001000\n",
      "Iter  5800 loss 26.859081 accuracy 0.24 lr 0.001000\n",
      "Iter  5900 loss 26.232319 accuracy 0.25 lr 0.001000\n",
      "Iter  6000 loss 43.967857 accuracy 0.21 lr 0.001000\n",
      "Iter  6100 loss 27.791801 accuracy 0.24 lr 0.001000\n",
      "Iter  6200 loss 35.630478 accuracy 0.21 lr 0.001000\n",
      "Iter  6300 loss 35.000214 accuracy 0.24 lr 0.001000\n",
      "Iter  6400 loss 27.281990 accuracy 0.27 lr 0.001000\n",
      "Iter  6500 loss 26.809160 accuracy 0.20 lr 0.001000\n",
      "Iter  6600 loss 22.342833 accuracy 0.23 lr 0.001000\n",
      "Iter  6700 loss 27.626587 accuracy 0.23 lr 0.001000\n",
      "Iter  6800 loss 32.027996 accuracy 0.16 lr 0.001000\n",
      "Iter  6900 loss 33.935806 accuracy 0.23 lr 0.001000\n",
      "Iter  7000 loss 30.001354 accuracy 0.21 lr 0.001000\n",
      "Iter  7100 loss 29.741455 accuracy 0.26 lr 0.001000\n",
      "Iter  7200 loss 33.760021 accuracy 0.18 lr 0.001000\n",
      "Iter  7300 loss 32.002747 accuracy 0.19 lr 0.001000\n",
      "Iter  7400 loss 28.209553 accuracy 0.23 lr 0.001000\n",
      "Iter  7500 loss 47.106834 accuracy 0.21 lr 0.001000\n",
      "Iter  7600 loss 30.568897 accuracy 0.23 lr 0.001000\n",
      "Iter  7700 loss 37.865334 accuracy 0.19 lr 0.001000\n",
      "Iter  7800 loss 26.770126 accuracy 0.20 lr 0.001000\n",
      "Iter  7900 loss 18.263176 accuracy 0.25 lr 0.001000\n",
      "Iter  8000 loss 23.768803 accuracy 0.20 lr 0.001000\n",
      "Iter  8100 loss 41.215263 accuracy 0.20 lr 0.001000\n",
      "Iter  8200 loss 18.445118 accuracy 0.26 lr 0.001000\n",
      "Iter  8300 loss 33.217819 accuracy 0.23 lr 0.001000\n",
      "Iter  8400 loss 26.754917 accuracy 0.26 lr 0.001000\n",
      "Iter  8500 loss 24.876282 accuracy 0.20 lr 0.001000\n",
      "Iter  8600 loss 42.484879 accuracy 0.19 lr 0.001000\n",
      "Iter  8700 loss 23.842846 accuracy 0.21 lr 0.001000\n",
      "Iter  8800 loss 39.841805 accuracy 0.21 lr 0.001000\n",
      "Iter  8900 loss 23.512743 accuracy 0.24 lr 0.001000\n",
      "Iter  9000 loss 42.820923 accuracy 0.21 lr 0.001000\n",
      "Iter  9100 loss 32.454884 accuracy 0.22 lr 0.001000\n",
      "Iter  9200 loss 42.237011 accuracy 0.18 lr 0.001000\n",
      "Iter  9300 loss 38.805813 accuracy 0.20 lr 0.001000\n",
      "Iter  9400 loss 42.063217 accuracy 0.15 lr 0.001000\n",
      "Iter  9500 loss 39.606895 accuracy 0.23 lr 0.001000\n",
      "Iter  9600 loss 27.556305 accuracy 0.25 lr 0.001000\n",
      "Iter  9700 loss 29.692875 accuracy 0.18 lr 0.001000\n",
      "Iter  9800 loss 42.808563 accuracy 0.17 lr 0.001000\n",
      "Iter  9900 loss 38.559204 accuracy 0.15 lr 0.001000\n",
      "Iter 10000 loss 38.861938 accuracy 0.22 lr 0.001000\n",
      "Iter 10100 loss 28.161617 accuracy 0.22 lr 0.001000\n",
      "Iter 10200 loss 33.469543 accuracy 0.25 lr 0.001000\n",
      "Iter 10300 loss 30.183788 accuracy 0.18 lr 0.001000\n",
      "Iter 10400 loss 31.003944 accuracy 0.21 lr 0.001000\n",
      "Iter 10500 loss 34.359390 accuracy 0.18 lr 0.001000\n",
      "Iter 10600 loss 37.293468 accuracy 0.19 lr 0.001000\n",
      "Iter 10700 loss 36.430046 accuracy 0.19 lr 0.001000\n",
      "Iter 10800 loss 43.671421 accuracy 0.18 lr 0.001000\n",
      "Iter 10900 loss 21.223576 accuracy 0.22 lr 0.001000\n",
      "Iter 11000 loss 38.955139 accuracy 0.19 lr 0.001000\n",
      "Iter 11100 loss 23.965931 accuracy 0.20 lr 0.001000\n",
      "Epoch 1 loss 36.068690 accuracy 0.20 val_aer 0.92 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 24.431526 accuracy 0.23 lr 0.001000\n",
      "Iter   200 loss 35.329784 accuracy 0.20 lr 0.001000\n",
      "Iter   300 loss 34.201675 accuracy 0.19 lr 0.001000\n",
      "Iter   400 loss 34.573952 accuracy 0.21 lr 0.001000\n",
      "Iter   500 loss 41.894669 accuracy 0.26 lr 0.001000\n",
      "Iter   600 loss 25.776276 accuracy 0.26 lr 0.001000\n",
      "Iter   700 loss 33.223869 accuracy 0.25 lr 0.001000\n",
      "Iter   800 loss 48.332767 accuracy 0.17 lr 0.001000\n",
      "Iter   900 loss 26.693817 accuracy 0.16 lr 0.001000\n",
      "Iter  1000 loss 40.776512 accuracy 0.20 lr 0.001000\n",
      "Iter  1100 loss 26.772295 accuracy 0.23 lr 0.001000\n",
      "Iter  1200 loss 28.000893 accuracy 0.19 lr 0.001000\n",
      "Iter  1300 loss 36.721046 accuracy 0.23 lr 0.001000\n",
      "Iter  1400 loss 39.730602 accuracy 0.23 lr 0.001000\n",
      "Iter  1500 loss 26.997431 accuracy 0.22 lr 0.001000\n",
      "Iter  1600 loss 17.427097 accuracy 0.25 lr 0.001000\n",
      "Iter  1700 loss 37.374050 accuracy 0.20 lr 0.001000\n",
      "Iter  1800 loss 26.943192 accuracy 0.24 lr 0.001000\n",
      "Iter  1900 loss 40.446754 accuracy 0.19 lr 0.001000\n",
      "Iter  2000 loss 33.186047 accuracy 0.21 lr 0.001000\n",
      "Iter  2100 loss 33.194939 accuracy 0.25 lr 0.001000\n",
      "Iter  2200 loss 25.839016 accuracy 0.20 lr 0.001000\n",
      "Iter  2300 loss 45.388367 accuracy 0.22 lr 0.001000\n",
      "Iter  2400 loss 35.298710 accuracy 0.24 lr 0.001000\n",
      "Iter  2500 loss 32.587219 accuracy 0.24 lr 0.001000\n",
      "Iter  2600 loss 45.184994 accuracy 0.21 lr 0.001000\n",
      "Iter  2700 loss 28.905670 accuracy 0.21 lr 0.001000\n",
      "Iter  2800 loss 36.318535 accuracy 0.22 lr 0.001000\n",
      "Iter  2900 loss 40.658318 accuracy 0.16 lr 0.001000\n",
      "Iter  3000 loss 38.801754 accuracy 0.22 lr 0.001000\n",
      "Iter  3100 loss 20.452543 accuracy 0.20 lr 0.001000\n",
      "Iter  3200 loss 26.356541 accuracy 0.21 lr 0.001000\n",
      "Iter  3300 loss 32.217304 accuracy 0.22 lr 0.001000\n",
      "Iter  3400 loss 38.802727 accuracy 0.18 lr 0.001000\n",
      "Iter  3500 loss 31.136173 accuracy 0.22 lr 0.001000\n",
      "Iter  3600 loss 33.235107 accuracy 0.21 lr 0.001000\n",
      "Iter  3700 loss 40.427589 accuracy 0.18 lr 0.001000\n",
      "Iter  3800 loss 34.454079 accuracy 0.22 lr 0.001000\n",
      "Iter  3900 loss 37.656723 accuracy 0.22 lr 0.001000\n",
      "Iter  4000 loss 41.551811 accuracy 0.17 lr 0.001000\n",
      "Iter  4100 loss 26.011482 accuracy 0.22 lr 0.001000\n",
      "Iter  4200 loss 23.705065 accuracy 0.27 lr 0.001000\n",
      "Iter  4300 loss 38.996033 accuracy 0.23 lr 0.001000\n",
      "Iter  4400 loss 34.583138 accuracy 0.18 lr 0.001000\n",
      "Iter  4500 loss 35.822300 accuracy 0.19 lr 0.001000\n",
      "Iter  4600 loss 32.065208 accuracy 0.23 lr 0.001000\n",
      "Iter  4700 loss 35.027557 accuracy 0.21 lr 0.001000\n",
      "Iter  4800 loss 37.277321 accuracy 0.17 lr 0.001000\n",
      "Iter  4900 loss 30.941887 accuracy 0.22 lr 0.001000\n",
      "Iter  5000 loss 31.625546 accuracy 0.18 lr 0.001000\n",
      "Iter  5100 loss 31.072964 accuracy 0.22 lr 0.001000\n",
      "Iter  5200 loss 31.450806 accuracy 0.21 lr 0.001000\n",
      "Iter  5300 loss 30.440918 accuracy 0.21 lr 0.001000\n",
      "Iter  5400 loss 26.886961 accuracy 0.24 lr 0.001000\n",
      "Iter  5500 loss 16.239962 accuracy 0.28 lr 0.001000\n",
      "Iter  5600 loss 26.596802 accuracy 0.19 lr 0.001000\n",
      "Iter  5700 loss 34.813511 accuracy 0.18 lr 0.001000\n",
      "Iter  5800 loss 38.677742 accuracy 0.20 lr 0.001000\n",
      "Iter  5900 loss 34.199303 accuracy 0.22 lr 0.001000\n",
      "Iter  6000 loss 39.503281 accuracy 0.17 lr 0.001000\n",
      "Iter  6100 loss 26.018963 accuracy 0.17 lr 0.001000\n",
      "Iter  6200 loss 34.167969 accuracy 0.21 lr 0.001000\n",
      "Iter  6300 loss 34.408745 accuracy 0.17 lr 0.001000\n",
      "Iter  6400 loss 38.989647 accuracy 0.21 lr 0.001000\n",
      "Iter  6500 loss 32.843853 accuracy 0.19 lr 0.001000\n",
      "Iter  6600 loss 39.661121 accuracy 0.18 lr 0.001000\n",
      "Iter  6700 loss 24.811943 accuracy 0.22 lr 0.001000\n",
      "Iter  6800 loss 27.888374 accuracy 0.23 lr 0.001000\n",
      "Iter  6900 loss 29.152096 accuracy 0.24 lr 0.001000\n",
      "Iter  7000 loss 32.552231 accuracy 0.22 lr 0.001000\n",
      "Iter  7100 loss 28.519075 accuracy 0.21 lr 0.001000\n",
      "Iter  7200 loss 32.459114 accuracy 0.23 lr 0.001000\n",
      "Iter  7300 loss 29.720200 accuracy 0.21 lr 0.001000\n",
      "Iter  7400 loss 25.404110 accuracy 0.22 lr 0.001000\n",
      "Iter  7500 loss 35.740784 accuracy 0.15 lr 0.001000\n",
      "Iter  7600 loss 33.285851 accuracy 0.21 lr 0.001000\n",
      "Iter  7700 loss 23.471109 accuracy 0.25 lr 0.001000\n",
      "Iter  7800 loss 27.581928 accuracy 0.24 lr 0.001000\n",
      "Iter  7900 loss 32.619812 accuracy 0.21 lr 0.001000\n",
      "Iter  8000 loss 37.144913 accuracy 0.18 lr 0.001000\n",
      "Iter  8100 loss 26.015457 accuracy 0.27 lr 0.001000\n",
      "Iter  8200 loss 43.876892 accuracy 0.19 lr 0.001000\n",
      "Iter  8300 loss 34.954109 accuracy 0.20 lr 0.001000\n",
      "Iter  8400 loss 32.267647 accuracy 0.19 lr 0.001000\n",
      "Iter  8500 loss 29.981094 accuracy 0.23 lr 0.001000\n",
      "Iter  8600 loss 26.758184 accuracy 0.19 lr 0.001000\n",
      "Iter  8700 loss 31.344460 accuracy 0.19 lr 0.001000\n",
      "Iter  8800 loss 37.871677 accuracy 0.21 lr 0.001000\n",
      "Iter  8900 loss 39.595371 accuracy 0.16 lr 0.001000\n",
      "Iter  9000 loss 30.751535 accuracy 0.19 lr 0.001000\n",
      "Iter  9100 loss 36.844719 accuracy 0.26 lr 0.001000\n",
      "Iter  9200 loss 23.429392 accuracy 0.21 lr 0.001000\n",
      "Iter  9300 loss 45.659134 accuracy 0.19 lr 0.001000\n",
      "Iter  9400 loss 42.985653 accuracy 0.20 lr 0.001000\n",
      "Iter  9500 loss 32.742054 accuracy 0.22 lr 0.001000\n",
      "Iter  9600 loss 35.294579 accuracy 0.20 lr 0.001000\n",
      "Iter  9700 loss 28.987091 accuracy 0.23 lr 0.001000\n",
      "Iter  9800 loss 25.312691 accuracy 0.26 lr 0.001000\n",
      "Iter  9900 loss 34.533596 accuracy 0.23 lr 0.001000\n",
      "Iter 10000 loss 36.840508 accuracy 0.20 lr 0.001000\n",
      "Iter 10100 loss 26.137482 accuracy 0.25 lr 0.001000\n",
      "Iter 10200 loss 36.625538 accuracy 0.21 lr 0.001000\n",
      "Iter 10300 loss 33.722034 accuracy 0.20 lr 0.001000\n",
      "Iter 10400 loss 38.951256 accuracy 0.21 lr 0.001000\n",
      "Iter 10500 loss 37.328854 accuracy 0.19 lr 0.001000\n",
      "Iter 10600 loss 43.385475 accuracy 0.15 lr 0.001000\n",
      "Iter 10700 loss 36.257767 accuracy 0.17 lr 0.001000\n",
      "Iter 10800 loss 30.004230 accuracy 0.22 lr 0.001000\n",
      "Iter 10900 loss 42.374207 accuracy 0.19 lr 0.001000\n",
      "Iter 11000 loss 50.892048 accuracy 0.16 lr 0.001000\n",
      "Iter 11100 loss 32.035942 accuracy 0.21 lr 0.001000\n",
      "Epoch 2 loss 33.252296 accuracy 0.21 val_aer 0.94 val_acc 0.19\n",
      "Computing training-set likelihood\n",
      "Computing dev-set likelihood\n",
      "Model saved in file: model.ckpt\n",
      "Shuffling training data\n",
      "Iter   100 loss 22.048010 accuracy 0.17 lr 0.001000\n",
      "Iter   200 loss 34.578384 accuracy 0.17 lr 0.001000\n",
      "Iter   300 loss 35.312901 accuracy 0.24 lr 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3f2243609395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# now we can start training!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training started..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mdev_AERs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_AERs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_likelihoods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_likelihoods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timbaumgartner/Documents/AI/NLP2/Projects.tmp/NLP2-Project3/neuralibm1trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m         }\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # some hyper-parameters\n",
    "    # tweak them as you wish\n",
    "    batch_size=16  # on CPU, use something much smaller e.g. 1-16\n",
    "    max_length=30\n",
    "    lr = 0.001\n",
    "    lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "    emb_dim = 64\n",
    "    mlp_dim = 128\n",
    "\n",
    "    # our model\n",
    "    # change context to : \"gate\", \"concat\", or \"col_discrete\". \"col_discrete is for T3\"\n",
    "    model = NeuralIBM1Model(\n",
    "        x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f, \n",
    "        batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess, context=\"concat\")\n",
    "\n",
    "    # our trainer\n",
    "    trainer = NeuralIBM1Trainer(\n",
    "        model, train_e_path, train_f_path, \n",
    "        dev_e_path, dev_f_path, dev_wa,\n",
    "        test_e_path, test_f_path, test_wa,\n",
    "        num_epochs=10, batch_size=batch_size, \n",
    "        max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess)\n",
    "\n",
    "    # now first TF needs to initialize all the variables\n",
    "    print(\"Initializing variables..\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # now we can start training!\n",
    "    print(\"Training started..\")\n",
    "    results = trainer.train()\n",
    "    dev_AERs, test_AERs, train_likelihoods, dev_likelihoods = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print (np.__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_AERs)+1), dev_AERs, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "ax2 = plt.plot(range(1, len(test_AERs)+1), test_AERs, label='test-set')\n",
    "handles.extend(ax2)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AER')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(train_likelihoods)+1), train_likelihoods, label='training-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_likelihoods)+1), dev_likelihoods, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # some hyper-parameters\n",
    "    # tweak them as you wish\n",
    "    batch_size=16  # on CPU, use something much smaller e.g. 1-16\n",
    "    max_length=30\n",
    "    lr = 0.001\n",
    "    lr_decay = 0.0  # set to 0.0 when using Adam optimizer (default)\n",
    "    emb_dim = 64\n",
    "    mlp_dim = 128\n",
    "\n",
    "    # our model\n",
    "    # change context to : \"gate\", \"concat\", or \"col_discrete\". \"col_discrete is for T3\"\n",
    "    model = NeuralIBM1Model(\n",
    "        x_vocabulary=vocabulary_e, y_vocabulary=vocabulary_f, \n",
    "        batch_size=batch_size, emb_dim=emb_dim, mlp_dim=mlp_dim, session=sess, context=\"gate\")\n",
    "\n",
    "    # our trainer\n",
    "    trainer = NeuralIBM1Trainer(\n",
    "        model, train_e_path, train_f_path, \n",
    "        dev_e_path, dev_f_path, dev_wa,\n",
    "        test_e_path, test_f_path, test_wa,\n",
    "        num_epochs=10, batch_size=batch_size, \n",
    "        max_length=max_length, lr=lr, lr_decay=lr_decay, session=sess)\n",
    "\n",
    "    # now first TF needs to initialize all the variables\n",
    "    print(\"Initializing variables..\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # now we can start training!\n",
    "    print(\"Training started..\")\n",
    "    results = trainer.train()\n",
    "    dev_AERs, test_AERs, train_likelihoods, dev_likelihoods = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_AERs)+1), dev_AERs, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "ax2 = plt.plot(range(1, len(test_AERs)+1), test_AERs, label='test-set')\n",
    "handles.extend(ax2)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AER')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(train_likelihoods)+1), train_likelihoods, label='training-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "handles = []\n",
    "ax1 = plt.plot(range(1, len(dev_likelihoods)+1), dev_likelihoods, label='dev-set')\n",
    "handles.extend(ax1)\n",
    "plt.legend(handles=handles)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
